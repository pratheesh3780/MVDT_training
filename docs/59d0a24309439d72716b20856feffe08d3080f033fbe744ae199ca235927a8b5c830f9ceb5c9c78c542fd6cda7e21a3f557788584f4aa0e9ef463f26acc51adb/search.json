[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"\n\n\nWelcome “Multivariate Data Analysis Tools Agricultural Research” training manual, specialized resource tailored agricultural researchers leveraging R programming language data analysis. realm agricultural research, multidimensional datasets hold keys improving crop yields, soil health, sustainable farming practices, manual equips essential knowledge skills unlock actionable insights. fundamental concepts practical applications, training explores techniques like PCA, Factor Analysis, Cluster Analysis, , within context agricultural research. power R fingertips, ’ll harness full potential agricultural data, making informed decisions drive innovation advancement field agriculture.\n\ncontent manual carefully designed ensure presented simple straightforward manner, making accessible individuals levels expertise. aim demystify complex concepts, providing clarity ease understanding anyone, regardless background, can grasp apply fundamental principles multivariate data analysis agricultural research confidence.\n","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"\n\n\nNote: training Manual published MeLoN (Module e-Learning & Online Notes) Department Agricultural Statistics . online version book free read .\n\nfeedback, please feel free contact Dr.Pratheesh P. Gopinath. E-mail: pratheesh.pg@kau.Thank !\n","code":""},{"path":"r-and-r-studio.html","id":"r-and-r-studio","chapter":"1 R and R studio","heading":"1 R and R studio","text":"training program tailored equip skills needed multivariate data analysis using R RStudio. introductory section, provide brief overview guide process installing R RStudio.","code":""},{"path":"r-and-r-studio.html","id":"r","chapter":"1 R and R studio","heading":"1.1 R","text":"R programming language statistical computing graphics supported R Core Team R Foundation Statistical Computing. Created statisticians Ross Ihaka Robert Gentleman. R implementation S programming language. R used among data miners, bioinformaticians statisticians data analysis developing statistical software. Users created packages augment functions R language.\nAccording user surveys studies scholarly literature databases, R one commonly used programming languages used data mining. official R software environment open-source free software environment within GNU package, available GNU General Public License. written primarily C, Fortran, R (partially self-hosting). R command line interface. Multiple third-party graphical user interfaces also available, RStudio, integrated development environment.\nFigure 1.1: R logo\n","code":""},{"path":"r-and-r-studio.html","id":"rstudio","chapter":"1 R and R studio","heading":"1.2 Rstudio","text":"RStudio integrated development environment (IDE) R. includes console, syntax-highlighting editor supports direct code execution, well tools plotting, history, debugging workspace management. RStudio available open source commercial editions runs desktop (Windows, Mac, Linux) browser connected RStudio Server. RStudio free open-source integrated development environment (IDE) R, programming language statistical computing graphics. JJ Allaire, creator programming language ColdFusion, founded RStudio. RStudio available two editions: RStudio Desktop, program run locally regular desktop application; RStudio Server, allows accessing RStudio using web browser running remote Linux server.RStudio written C++ programming language uses Qt framework graphical user interface. Work RStudio started around December 2010, first public beta version (v0.92) officially announced February 2011.\nFigure 1.2: R studio logo\nTypical RStudio window four panes explained \nFigure 1.3: R studio window\nConsoleThis action happens. authentic R code typed ‘>’ prompt executed pressing ‘Enter’ generate output. going type single call function start app data analysis.Source EditorThis R scripts (collection code) can created edited. New R script can opened clicking File –> New File –> R Script using short cut ctrl+shift+N. can type codes . run code console execute, place cursor line code written press ctrl+enter highlight code wish evaluate clicking “Run” button top right Source. can save R codes, written script.Environment|History|ConnectionsAll data objects (vectors, matrices, dataframes) defined current R session listed Environment tab panel. data objects, may also examine details like quantity observations rows. clickable options available tab, Import Dataset, launch graphical user interface (GUI) inputting data R.panel’s History tab provides history code previously evaluated Console.Environment / History panel helpful become accustomed R. However, can ignore right now. can simply just reduce window clicking minimise button panel’s upper right corner wish clear space screen.Files|Plots|Packages|Help|ViewerYou can find collection useful information Files|Plots|Packages|Help panel. Let’s examine tab detail:\n- file directory hard disc accessible files panel. can utilise “Files” panel set working directory clicking “” “Set Working Directory” ’ve navigated folder wish read save files.plots displayed Plots panel. buttons export plot pdf jpg open plot separate window.\ninterested install button pane, install packages required analysis.","code":""},{"path":"r-and-r-studio.html","id":"installing-r","chapter":"1 R and R studio","heading":"1.3 Installing R","text":"Follow steps correct order installation R","code":""},{"path":"r-and-r-studio.html","id":"first-install-r-latest-version","chapter":"1 R and R studio","heading":"1.3.1 First install R latest version","text":"install R Windows OS:Go CRAN website.Click “Download R Windows”.Click “install R first time”.\nClick “Download R-4.3.1 windows” link download R executable (.exe) file.(time writing manual R version 4.3.1, may change future)downloading file. Run R executable file double clicking downloaded file start installation, allow app make changes device.Select installation language.Follow installation instructions.Click next wait installation complete.Click “Finish” exit complete installation setup.can now run R start menu shortcut created desktop","code":""},{"path":"r-and-r-studio.html","id":"installing-rstudio","chapter":"1 R and R studio","heading":"1.4 Installing Rstudio","text":"installing latest version R. Go Rstudio website.\nFigure 1.4: FALSE\nNow like completed installation R, complete installation setup Rstudio. installation can now run R start menu shortcut created desktop.Now ready explore world data analysis unleash potential R RStudio unlock valuable insights datasets. Happy Statistics R!","code":""},{"path":"r-and-r-studio.html","id":"the-above-descriptions-are-sourced-from","chapter":"1 R and R studio","heading":"The above descriptions are sourced from:","text":"R Project Statistical ComputingRStudioWikipedia R (programming language)Wikipedia RStudio","code":""},{"path":"import.html","id":"import","chapter":"2 Importing data files in R","heading":"2 Importing data files in R","text":"previous chapter discussed basics R\nprogramming including installation, launching, basic data types \narithmetic functions. , learn import data R. \nimportant ensure data well prepared importing\nR avoid errors.","code":""},{"path":"import.html","id":"preparing-your-file","chapter":"2 Importing data files in R","heading":"2.1 Preparing your file","text":"File can prepared MS ExcelFile can prepared MS ExcelUse first row column headers (column names). Generally,\ncolumns represent variables.Use first row column headers (column names). Generally,\ncolumns represent variables.Use first column row names. Generally rows represent\nobservations.Use first column row names. Generally rows represent\nobservations.Make sure row name unique. case \nanalysis experiments , row name treatment name,\nrepeated replicationMake sure row name unique. case \nanalysis experiments , row name treatment name,\nrepeated replicationColumn names compatible R naming conventions.","code":""},{"path":"import.html","id":"naming-conventions","chapter":"2 Importing data files in R","heading":"2.1.1 Naming conventions:","text":"Avoid names blank spaces. Bad column name Sepal width; Good\nconvention Sepal_widthAvoid names blank spaces. Bad column name Sepal width; Good\nconvention Sepal_widthAvoid names special symbols: ?, $, *, +, #, (, ), -, /, }, {,\n|, >, < etc. underscore can used.Avoid names special symbols: ?, $, *, +, #, (, ), -, /, }, {,\n|, >, < etc. underscore can used.Avoid beginning variable names number. Use letter instead.\nGood column names: obs_100m x100m. Bad column name: 100mAvoid beginning variable names number. Use letter instead.\nGood column names: obs_100m x100m. Bad column name: 100mColumn names must unique. Duplicated names allowed.Column names must unique. Duplicated names allowed.R case sensitive. means Name, NAME name, naMe \ntreated different.R case sensitive. means Name, NAME name, naMe \ntreated different.Avoid blank rows dataAvoid blank rows dataDelete comments fileDelete comments fileReplace missing values NA (denotes Available)Replace missing values NA (denotes Available)column containing date, use four digit format.\nGood format: 01/01/2016. Bad format: 01/01/16If column containing date, use four digit format.\nGood format: 01/01/2016. Bad format: 01/01/16A final good looking file\nFigure 2.1: example file\n","code":""},{"path":"import.html","id":"saving-file","chapter":"2 Importing data files in R","heading":"2.1.2 Saving file","text":"recommend save file .csv (comma separated value file)\nformat.CSV?usual file save MS Excel saved XLS files XLSX\nfiles. Workbook files Microsoft Excel 97 2003 known \nXLS files. XLSX extension used later versions Excel. \ndata worksheets workbook, including formatting,\ncharts, graphics, calculations, , contained XLS \nXLSX file formats.Comma Separated Values (CSV) format plain text format \nvalues separated commas, whereas Excel Sheets binary file\nformat (XLS) contains information worksheets file,\nincluding content formatting. spreadsheet programme,\nincluding Microsoft Excel, Open Office, Google Sheets, etc., can open\nCSV files. straightforward text editor can also used open CSV\nfiles. straightforward compatible majority \nplatforms, prevalent well-liked file format storing\naccessing data. certain drawbacks simplicity.\nCSV files can contain single sheet without formatting \nformulas.CSV files supported almost data upload interfaces,\nExcel (XLS XLSX) file types preferable storing \ncomplicated data. CSV file format may advantageous \nintend move data platforms export import \ninterfaces.","code":""},{"path":"import.html","id":"how-to-save-as-csv","chapter":"2 Importing data files in R","heading":"2.1.3 How to save as csv","text":"\"File name\" section \"Save \" tab, can select\n\"Save type\" change \"CSV (Comma delimited) (*.csv).\nFigure 2.2: save csv\n\nFigure 2.3: save csv (1)\n","code":""},{"path":"import.html","id":"importing-data-set-in-rstudio","chapter":"2 Importing data files in R","heading":"2.2 Importing Data set in Rstudio","text":"import csv file Rstudioclick File click Import Dataset select Text (base)\nFigure 2.4: Importing data set\nSelect file click open\nFigure 2.5: Import Dataset dialogue box\nImport Dataset dialogue box can change name \ndataset Box Name. Heading radio button default\n‘yes’. Click import. dataset now imported ready \nwork .","code":""},{"path":"import.html","id":"alternate-methods","chapter":"2 Importing data files in R","heading":"2.2.1 Alternate methods","text":"","code":""},{"path":"import.html","id":"importing-csv-files","chapter":"2 Importing data files in R","heading":"2.2.1.1 Importing csv files","text":"Data can also imported using read.csv() function R.\nread.csv('path file')","code":"\n# example\n\nmy_data<-read.csv(file = 'csv/usarrests.csv')\n\n# here now the data set usarrests.csv stored in folder csv is stored in the name my_data\n\n# You can now directly do operations on the my_data\n\nsummary(my_data)##       X                 Murder          Assault         UrbanPop    \n##  Length:50          Min.   : 0.800   Min.   : 45.0   Min.   :32.00  \n##  Class :character   1st Qu.: 4.075   1st Qu.:109.0   1st Qu.:54.50  \n##  Mode  :character   Median : 7.250   Median :159.0   Median :66.00  \n##                     Mean   : 7.788   Mean   :170.8   Mean   :65.54  \n##                     3rd Qu.:11.250   3rd Qu.:249.0   3rd Qu.:77.75  \n##                     Max.   :17.400   Max.   :337.0   Max.   :91.00  \n##       Rape      \n##  Min.   : 7.30  \n##  1st Qu.:15.07  \n##  Median :20.10  \n##  Mean   :21.23  \n##  3rd Qu.:26.18  \n##  Max.   :46.00"},{"path":"import.html","id":"importing-excel-files","chapter":"2 Importing data files in R","heading":"2.2.1.2 Importing excel files","text":"import xlsx file, need package xlsx","code":"\nlibrary(xlsx)  \n\ndf <-read.xlsx(\"path/file.xlsx\", n)\n\n# n is n-th worksheet to import"},{"path":"principal-component-analysis-pca.html","id":"principal-component-analysis-pca","chapter":"3 Principal Component Analysis (PCA)","heading":"3 Principal Component Analysis (PCA)","text":"","code":""},{"path":"principal-component-analysis-pca.html","id":"what-is-pca","chapter":"3 Principal Component Analysis (PCA)","heading":"3.1 What is PCA?","text":"Principal component analysis (PCA) technique transforms high-dimensions data lower-dimensions retaining much information possible. PCA invented 1909 Karl Pearson, analogue principal axis theorem mechanics; later independently developed named Harold Hotelling 1930s.(Johnson Wichern 2014)Drawing meaningful inferences high-dimensional data can challenging, humans naturally excel visualizing comprehending information two dimensions. PCA, powerful technique, aids transforming multi-dimensional data manageable form reducing dimensionality. simplification facilitates easier visualization analysis, ultimately enhancing ability extract valuable insights complex datasets.PCA valuable tool social science agricultural research. works transforming multi-dimensional data lower-dimensional space retaining much variance data possible. essence, PCA identifies significant dimensions “principal components” data, effectively reducing complexity.\nidea PCA simple — reduce number variables data set, preserving much information possible.","code":""},{"path":"principal-component-analysis-pca.html","id":"when-to-use-pca","chapter":"3 Principal Component Analysis (PCA)","heading":"3.2 When to Use PCA","text":"Dimensionality Reduction: Use PCA high-dimensional dataset many features (variables) want reduce dimensionality. can help cases many variables work efficiently.Data Visualization: PCA effective need visualize high-dimensional data. projecting data onto lower-dimensional space, can create scatter plots, heatmaps, visualizations easier interpret.Minimum data set: Principal components can used eliminate data sets identify minimum data set experimentation.Noise Reduction: dataset contains noisy redundant features, PCA can help capturing important information eliminating less relevant components.Multicollinearity: dataset multicollinearity issues (high correlations variables), PCA can help reduce interdependencies, making models stable interpretable.Sample size (n) least equal number dimensions (n ≥ p)","code":""},{"path":"principal-component-analysis-pca.html","id":"when-not-to-use-pca","chapter":"3 Principal Component Analysis (PCA)","heading":"3.3 When Not to Use PCA","text":"sample size less number dimensions (n < p)Non-Linear Relationships: PCA based linear transformations may effective data contains complex non-linear relationships. cases, techniques like kernel PCA non-linear dimensionality reduction methods might appropriate.Small Dimensionality: already low-dimensional dataset important variables, applying PCA might provide significant benefits even lead information loss.Loss Variability Information: PCA aims maximize variance capture, may desirable cases. preserving characteristics data important (e.g., categorical information), dimensionality reduction techniques considered.","code":""},{"path":"principal-component-analysis-pca.html","id":"how-pca-is-done","chapter":"3 Principal Component Analysis (PCA)","heading":"3.4 How PCA is done","text":"training program, primarily focused practical application PCA rather delving theoretical aspects. aim explore PCA can effectively utilized understand interpret results meaningful manner. usual procedure performing follows information:-Standardize range continuous initial variablesStandardize range continuous initial variablesCompute covariance matrix identify correlationsCompute covariance matrix identify correlationsCompute eigen vectors eigen values covariance matrix identify principal componentsCompute eigen vectors eigen values covariance matrix identify principal componentsCreate feature vector decide principal components keepCreate feature vector decide principal components keepRecast data along principal components axesRecast data along principal components axes","code":""},{"path":"principal-component-analysis-pca.html","id":"what-is-principal-component","chapter":"3 Principal Component Analysis (PCA)","heading":"3.5 What is Principal Component","text":"Principal components new variables constructed linear combinations initial variables. combinations done way new variables (.e. principal components) uncorrelated information within initial variables included first components. , idea 10-dimensional data gives 10 principal components, PCA tries put maximum possible information first component, maximum remaining information second .example scree plot shown can see 5 principal components 5-dimensional data corresponding variance explained.\nFigure 3.1: Scree Plot: Principal components percentage variance explained\n","code":""},{"path":"principal-component-analysis-pca.html","id":"how-pca-constructs-the-principal-components","chapter":"3 Principal Component Analysis (PCA)","heading":"3.6 How PCA Constructs the Principal Components","text":"Number principal components equal number variables data, principal components constructed manner first principal component accounts largest possible variance data set. example, see Figure 1.2 , can see scatter plot assumed data set, can guess first principal component ? Yes, ’s approximately line matches purple marks goes origin ’s line projection points (red dots) spread . mathematically speaking, ’s line maximizes variance (average squared distances projected points (red dots) origin).\nFigure 3.2: Concept PCA\nlines (PCs) identified using linear algebra concepts Eigen vectors eigen values calculated covariance matrix order determine principal components data. going much theoretical details.","code":""},{"path":"principal-component-analysis-pca.html","id":"practical-example","chapter":"3 Principal Component Analysis (PCA)","heading":"3.7 PRACTICAL EXAMPLE","text":"using Usarrests dataset R explain PCA coming sessions.“USarrests” dataset R built-dataset offers insights crime arrests across 50 states United States 1973. comprises four key variables: murder rate, assault rate, percentage population living urban areas, rape rate.can View download datasets ","code":""},{"path":"principal-component-analysis-pca.html","id":"analysis","chapter":"3 Principal Component Analysis (PCA)","heading":"3.8 Analysis","text":"First prepare data set save csv file. import data set R. See chapter 2 know save csv file import R. Also can directly use code import dataset computer R.Please note path copied system format C:\\Users\\HP\\Documents\\, change format C:/Users/HP/Documents/ R.\n(#fig:data_imp)Data set importing R Rstudio\nusing package factoextra performing analysisInstall required packages:Follow codes PCA analysiswe obtain descriptive statistics dataset moving .","code":"\ndata <- read.csv(\"path to your file\", row.names=1)\ninstall.packages(\"factoextra\")  \nlibrary(factoextra) \n\n#import your csv and save it in the name \"data\".\n\n#this will display the first 6 rows of your data\nhead(data, n = 6)##            Murder Assault UrbanPop Rape\n## Alabama      13.2     236       58 21.2\n## Alaska       10.0     263       48 44.5\n## Arizona       8.1     294       80 31.0\n## Arkansas      8.8     190       50 19.5\n## California    9.0     276       91 40.6\n## Colorado      7.9     204       78 38.7\nsummary(data)##      Murder          Assault         UrbanPop          Rape      \n##  Min.   : 0.800   Min.   : 45.0   Min.   :32.00   Min.   : 7.30  \n##  1st Qu.: 4.075   1st Qu.:109.0   1st Qu.:54.50   1st Qu.:15.07  \n##  Median : 7.250   Median :159.0   Median :66.00   Median :20.10  \n##  Mean   : 7.788   Mean   :170.8   Mean   :65.54   Mean   :21.23  \n##  3rd Qu.:11.250   3rd Qu.:249.0   3rd Qu.:77.75   3rd Qu.:26.18  \n##  Max.   :17.400   Max.   :337.0   Max.   :91.00   Max.   :46.00\n# Do PCA using prcomp function in factoextra\npca_res <- prcomp(data, scale = TRUE)\n# scale = TRUE will standardise the variables (x-mean(x)/sd(x))\nsummary(pca_res)## Importance of components:\n##                           PC1    PC2     PC3     PC4\n## Standard deviation     1.5749 0.9949 0.59713 0.41645\n## Proportion of Variance 0.6201 0.2474 0.08914 0.04336\n## Cumulative Proportion  0.6201 0.8675 0.95664 1.00000"},{"path":"principal-component-analysis-pca.html","id":"deciding-on-the-number-of-pcs","chapter":"3 Principal Component Analysis (PCA)","heading":"3.9 Deciding on the number of PCs","text":"","code":""},{"path":"principal-component-analysis-pca.html","id":"scree-plot","chapter":"3 Principal Component Analysis (PCA)","heading":"3.9.1 Scree Plot","text":"scree plot helps decide many principal components retain PCA analysis. choice number components can vary depending specific goals, ’s often based combination statistical criteria, explained variance elbow point, well domain knowledge interpretability.key inferences can make plot:Explained Variance: scree plot displays proportion total variance explained principal component. Inferences can made examining much variance explained component. components left contribute variance, right contribute less.Elbow Point: Look “elbow” point explained variance sharply decreases. often good indicator number principal components retain. point just explained variance starts level can suitable choice. ’s point adding components doesn’t explain much additional variance.Cumulative Variance: can also examine cumulative explained variance. scree plot may show cumulative curve increases components added. can look point cumulative variance reaches satisfactory level (e.g., 70%, 80%, 90%) determine number components retain.Interpretability: Consider interpretability components. Sometimes, might choose retain components even explain less variance meaningful interpretations context.Domain Knowledge: Always consider domain subject matter knowledge deciding number components retain. Sometimes, context analysis may dictate number components practically meaningful.","code":"\n# Visualize eigenvalues (scree plot)\nplot1 <- fviz_eig(pca_res,addlabels = TRUE)\nplot1\n# Drawing an elbow point if needed\npca.var =pca_res$sdev ^2\nvar.ratio=pca.var/sum(pca.var)\nplot(var.ratio , xlab=\" Principal Component \", ylab=\" Proportion of\nVariance Explained \", ylim=c(0,1) ,type=\"b\")"},{"path":"principal-component-analysis-pca.html","id":"eigen-values","chapter":"3 Principal Component Analysis (PCA)","heading":"3.9.2 Eigen Values","text":"“Eigenvalues” represent variance explained principal component (PC) PCA. Percentage variance ratio eigen value principal component sum eigen values PCs. important note first two PCs atleast explain 80% variance data.Eigenvalues can used determine number principal components retain PCA (Kaiser 1961):eigenvalue > 1 indicates PCs account variance accounted one original variables standardized data. commonly used cutoff point PCs retained. holds true data standardized.can also select number principal components Principal Component Analysis (PCA) based desired level explained variance. instance, want retain 80% total variance explained, can choose number components achieves level.","code":"\n# Eigenvalues\neig.val <- get_eigenvalue(pca_res)\neig.val##       eigenvalue variance.percent cumulative.variance.percent\n## Dim.1  2.4802416        62.006039                    62.00604\n## Dim.2  0.9897652        24.744129                    86.75017\n## Dim.3  0.3565632         8.914080                    95.66425\n## Dim.4  0.1734301         4.335752                   100.00000\n#Now we will take the PCA results keeping first two pricipal components only\nres.pca <- prcomp(data, scale = TRUE, rank =2)\n#here rank =2 will keep only two PCs"},{"path":"principal-component-analysis-pca.html","id":"accessing-the-pca-results","chapter":"3 Principal Component Analysis (PCA)","heading":"3.10 Accessing the PCA results","text":"Please note results discussed Dim1 , Dim2 etc denotes Principal Component1(PC1), PC2 etc respectively. now discuss terms PCA can interpreted get meaningful insights.","code":""},{"path":"principal-component-analysis-pca.html","id":"pc-loadings","chapter":"3 Principal Component Analysis (PCA)","heading":"3.10.1 PC Loadings","text":"Loadings coefficients linear combination predicting variable (standardized) components. Loadings represent weights assigned original variable linear combination forms principal component. weights indicate importance variable creating component. statistical language loadings eigenvectors scaled square roots respective eigenvalues.Positive loadings indicate positive relationship variable component, suggesting increase variable associated increase component’s value. Negative loadings indicate negative relationship, meaning increase variable corresponds decrease component’s value. magnitude loading reflects strength relationship. Larger loadings indicate variable substantial impact component. Loadings typically standardized mean 0 standard deviation 1, ensuring variables different scales directly comparable.first loading vector places approximately equal weight Assault, Murder, Rape, much less weight UrbanPop. Hence component roughly corresponds measure overall rates serious crimes. loadings negative can assume states scoring lesser values PC1 higher crime rate.","code":"\n#getting PC loadings\nres.pca$rotation##                 PC1        PC2\n## Murder   -0.5358995 -0.4181809\n## Assault  -0.5831836 -0.1879856\n## UrbanPop -0.2781909  0.8728062\n## Rape     -0.5434321  0.1673186"},{"path":"principal-component-analysis-pca.html","id":"variable-coordinates","chapter":"3 Principal Component Analysis (PCA)","heading":"3.10.2 Variable coordinates","text":"coordinates provides insights relative positions variables PCA space relate principal components geometrically. Absolute value measures gives strength association variable particular PCs. can avoid final tables, presenting results. coordinates already represented biplots.","code":"\n# Results for Variables\nres.var <- get_pca_var(res.pca)\nres.var$coord          # Coordinates  ##               Dim.1      Dim.2      Dim.3       Dim.4\n## Murder   -0.8439764 -0.4160354 -0.3200012 -0.17415116\n## Assault  -0.9184432 -0.1870211 -0.3482359 -0.07828649\n## UrbanPop -0.4381168  0.8683282 -0.1661159  0.36347960\n## Rape     -0.8558394  0.1664602 -0.3244991  0.06967974"},{"path":"principal-component-analysis-pca.html","id":"variable-contributions","chapter":"3 Principal Component Analysis (PCA)","heading":"3.10.3 Variable contributions","text":"measures provides percentage contributions variable principal component. indicates proportion variance explained variable principal component. Higher values suggest variable substantial influence formation respective component. helps identify variables contribute significantly variance explained component useful variable selection interpretation.contributions reflect idea loadings convey. can see higher contribution Assault, Murder, Rape PC1 higher contribution urban population PC2.","code":"\nres.var$contrib        # Contributions to the PCs  ##              Dim.1     Dim.2     Dim.3     Dim.4\n## Murder   28.718825 17.487524 28.718825 17.487524\n## Assault  34.010315  3.533859 34.010315  3.533859\n## UrbanPop  7.739016 76.179065  7.739016 76.179065\n## Rape     29.531844  2.799553 29.531844  2.799553"},{"path":"principal-component-analysis-pca.html","id":"cos2-representation","chapter":"3 Principal Component Analysis (PCA)","heading":"3.11 cos2 representation","text":"Cosine Squared (Cos²) Values: cos2 values provides cosine squared values variable respect principal component. Cosine squared values measure much variable’s variance explained corresponding principal component. cos2 values typically range 0 1. value 1 indicates variable perfectly aligned principal component, meaning variable’s entire variance explained component. value 0 indicates variable orthogonal component.\nHigher cos2 values indicate variable well-represented principal component. variables contribute significantly explanation variance along component. Lower values suggest variable less alignment component less influential explaining variance.can exclude values final table presenting results.","code":"\nres.var$cos2           # Quality of representation ##              Dim.1     Dim.2      Dim.3       Dim.4\n## Murder   0.7122962 0.1730854 0.10240075 0.030328628\n## Assault  0.8435380 0.0349769 0.12126826 0.006128774\n## UrbanPop 0.1919463 0.7539938 0.02759448 0.132117419\n## Rape     0.7324611 0.0277090 0.10529968 0.004855266"},{"path":"principal-component-analysis-pca.html","id":"measures-of-indviduals","chapter":"3 Principal Component Analysis (PCA)","heading":"3.11.1 Measures of Indviduals","text":"Similar measures calculated individuals also. individuals higher values PCs can identified insights can drawn based .\nexample states high negative values PC1 high crime rate states high values PC2 higher level urbanization.\nprincipal components scores vector 50 states(indviduals) can viewed using following code.California, Nevada Florida, high crime rates large negative scores first component, states like North Dakota, positive scores first component, low crime rates.California also high score second component, indicating high level urbanization, opposite true states like Mississippi.can find contributions, cos2 coordinates individuals, depth analysis required study, using code .","code":"\n# principal components scores vector for all 50 states(indviduals)\nres.pca$x##                        PC1         PC2\n## Alabama        -0.97566045 -1.12200121\n## Alaska         -1.93053788 -1.06242692\n## Arizona        -1.74544285  0.73845954\n## Arkansas        0.13999894 -1.10854226\n## California     -2.49861285  1.52742672\n## Colorado       -1.49934074  0.97762966\n## Connecticut     1.34499236  1.07798362\n## Delaware       -0.04722981  0.32208890\n## Florida        -2.98275967 -0.03883425\n## Georgia        -1.62280742 -1.26608838\n## Hawaii          0.90348448  1.55467609\n## Idaho           1.62331903 -0.20885253\n## Illinois       -1.36505197  0.67498834\n## Indiana         0.50038122  0.15003926\n## Iowa            2.23099579  0.10300828\n## Kansas          0.78887206  0.26744941\n## Kentucky        0.74331256 -0.94880748\n## Louisiana      -1.54909076 -0.86230011\n## Maine           2.37274014 -0.37260865\n## Maryland       -1.74564663 -0.42335704\n## Massachusetts   0.48128007  1.45967706\n## Michigan       -2.08725025  0.15383500\n## Minnesota       1.67566951  0.62590670\n## Mississippi    -0.98647919 -2.36973712\n## Missouri       -0.68978426  0.26070794\n## Montana         1.17353751 -0.53147851\n## Nebraska        1.25291625  0.19200440\n## Nevada         -2.84550542  0.76780502\n## New Hampshire   2.35995585  0.01790055\n## New Jersey     -0.17974128  1.43493745\n## New Mexico     -1.96012351 -0.14141308\n## New York       -1.66566662  0.81491072\n## North Carolina -1.11208808 -2.20561081\n## North Dakota    2.96215223 -0.59309738\n## Ohio            0.22369436  0.73477837\n## Oklahoma        0.30864928  0.28496113\n## Oregon         -0.05852787  0.53596999\n## Pennsylvania    0.87948680  0.56536050\n## Rhode Island    0.85509072  1.47698328\n## South Carolina -1.30744986 -1.91397297\n## South Dakota    1.96779669 -0.81506822\n## Tennessee      -0.98969377 -0.85160534\n## Texas          -1.34151838  0.40833518\n## Utah            0.54503180  1.45671524\n## Vermont         2.77325613 -1.38819435\n## Virginia        0.09536670 -0.19772785\n## Washington      0.21472339  0.96037394\n## West Virginia   2.08739306 -1.41052627\n## Wisconsin       2.05881199  0.60512507\n## Wyoming         0.62310061 -0.31778662\nres.ind <- get_pca_ind(res.pca)\nres.ind$coord          # Coordinates\nres.ind$contrib        # Contributions to the PCs\nres.ind$cos2           # Quality of representation "},{"path":"principal-component-analysis-pca.html","id":"biplot-indviduals","chapter":"3 Principal Component Analysis (PCA)","heading":"3.12 Biplot (Indviduals)","text":"“Biplot Individuals” graphical representation used Principal Component Analysis (PCA) multivariate statistical methods visualize individual data points observations related underlying patterns variables dataset. examining biplot, analysts researchers can gain insights various aspects data, including clustering patterns among individual observations.Interpretation Biplot indvidualsIndividuals’ Position: point biplot represents individual observation dataset. positions points plot show individual relates principal components. Individuals closer plot similar terms relationships principal components.Color Coding: points representing individuals may color-coded based “cos2” values, indicate quality representation individual plot. Higher “cos2” values indicate individual’s position plot strongly associated principal components.“cos2” Biplot:\ncontext PCA biplot, “cos2” represents square cosine angle individual’s position (point) variable’s arrow plot. quantifies quality representation individual plot.High “cos2” values suggest individual’s position plot well-represented variables principal components.High “cos2” values suggest individual’s position plot well-represented variables principal components.Low “cos2” values suggest individual’s position may well-represented may noisy plotLow “cos2” values suggest individual’s position may well-represented may noisy plot","code":"\n#Graph of individuals. Individuals with a similar profile are grouped together (Biplot)\nbiplot1<-fviz_pca_ind(res.pca,\n             col.ind = \"cos2\", # Color by the quality of representation\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE     # Avoid text overlapping\n             )\nbiplot1"},{"path":"principal-component-analysis-pca.html","id":"biplot-variables","chapter":"3 Principal Component Analysis (PCA)","heading":"3.13 Biplot (variables)","text":"biplot, variables represented arrows vectors, plot allows examination variables associated principal components.Interpretation PCA Variable BiPlot:Variable Positions: point variable plot represents variable dataset. positions points plot show variable relates principal components. Variables closer plot similar terms relationships principal components.Direction Arrows: Arrows variable plot represent variables contribution principal components. direction arrows indicates variable associated principal component. Variables pointing direction positively correlated corresponding component, pointing opposite directions negatively correlated. helps understand variables move direction component move opposite direction.Arrow Length: length arrows indicates strength relationship variables principal components. Longer arrows represent variables higher contribution component.Color Coding: points representing variables may color-coded based contributions principal components. Higher contributions typically associated darker color.","code":"\n#Graph of variables. \nbiplot2<-fviz_pca_var(res.pca,\n             col.var = \"contrib\", # Color by contributions to the PC\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE     # Avoid text overlapping\n             )\nbiplot2"},{"path":"principal-component-analysis-pca.html","id":"biplot-indviduals-and-variables","chapter":"3 Principal Component Analysis (PCA)","heading":"3.14 Biplot (Indviduals and variables)","text":"biplot combines individual data points represented points plot variables represented arrows vectors. provides powerful way assess relationships individual observations variables, offering insights patterns, clusters, associations within data. examining positions individuals directions variable vectors, analysts can gain holistic understanding data’s underlying structure, making valuable tool dimensionality reduction data exploration.Interpretation PCA BiPlot (Individuals Variables):Distance Origin (Center): distance individual point origin (center) plot represents contribution explained variance principal components. Points origin stronger influence principal components, points closer origin weaker influence. words, individuals origin better represented principal components.Projection onto Arrows: position individual point relative arrows tells individual influenced variables. individual point projected close tip arrow, indicates individual’s profile strongly influenced particular variable represented arrow.Alignment Points Arrows: direction arrows position individuals align, suggests variables individuals positively correlated corresponding principal component. words, variables point direction individual points contribute positively principal component.Opposite Direction: individual point arrow opposite directions, indicates negative correlation individual variable, implying variable contributes negatively principal component individual.summary, relationship position individuals direction arrows PCA biplot provides insights individual observations relate variables principal components. biplot helps understand strength direction relationships, allowing identify variables influence components individuals affected variables.biplot shows 50 states mapped according 2 principal components. vectors PCA 4 variables also plotted.states California, Nevada Florida,seen towards extreme left, indicating high negative value PC1. .e. states high crime rates.states like North Dakota seen towards right high positive scores first component, indicative low crime rates.general can divide quadrants biplot example follows:States Quadrant towards right: Low crime rate high urbanizationStates Quadrant II towards left: High crime rate high urbanizationStates Quadrant III towards bottom left: High crime rate low urbanizationStates Quadrant IV towards bottom right: Low crime rate low urbanizationTowards center: States moderate crime rate urbanization\nFigure 3.3: Quadrants biplot\nUsing code can save plots required resolution. changing resolution change value res code .","code":"\n#Biplot of individuals and variables\nbiplot3<- fviz_pca_biplot(res.pca, repel = TRUE,\n                col.var = \"#2E9FDF\", # Variables color\n                col.ind = \"#696969\"  # Individuals color\n                )\nbiplot3\n# Save the scree plot as a PNG file\n  png(\"eigenvalues_plot.png\", width = 800, height = 600, units = \"px\", res = 100)\n  plot1\n  dev.off() # Close the PNG device  \n\n  # Save the biplot of indviduals a PNG file\n  png(\"bi_plot.png\", width = 800, height = 600, units = \"px\", res = 100)\n  biplot1\n  dev.off() # Close the PNG device  \n  \n  # Save the biplot of variables a PNG file\n  png(\"bi_plot2.png\", width = 800, height = 600, units = \"px\", res = 100)\n  biplot2\n  dev.off() # Close the PNG device  \n  \n  # Save the biplot of indviduals and variables a PNG file\n  png(\"bi_plot3.png\", width = 800, height = 600, units = \"px\", res = 100)\n  biplot3\n  dev.off() # Close the PNG device  "},{"path":"principal-component-analysis-pca.html","id":"do-pca-in-our-inbuilt-app","chapter":"3 Principal Component Analysis (PCA)","heading":"3.15 Do PCA in our inbuilt app","text":"Good news!\nadded app can simply upload csv get results running cloud server!.","code":""},{"path":"exploratory-factor-analysis.html","id":"exploratory-factor-analysis","chapter":"4 Exploratory Factor Analysis","heading":"4 Exploratory Factor Analysis","text":"Exploratory Factor Analysis (EFA) statistical technique used field multivariate data analysis identify underlying patterns latent variables (hidden variables) explain correlations covariations among set observed variables. dimensionality reduction method commonly employed fields like psychology, sociology, social sciences, well agricultural extension research areas data reduction simplification needed (Johnson Wichern 2014).Latent variable: unobservable hidden variable directly measured inferred estimated based impact observed measurable variables. Latent variables often used statistical modeling explain patterns, relationships, variability data. represent underlying constructs, concepts, factors influence observed data directly observed measured.primary goal EFA uncover latent factors constructs responsible observed patterns data. latent factors unobservable, help explain common variance among observed variables. EFA assume pre-defined relationships variables used data exploration hypothesis generation.Imagine data 50 students, ’ve recorded marks subjects: English, Hindi, French, Physics, Maths, Statistics. want understand underlying factors influence subject scores. Factor analysis can help us understand underlying factors may contribute students’ academic performance. case, may identifies factors related language humanities skills (e.g., English, Hindi, French) analytical skills (e.g., Physics, Maths, Statistics).two types factor analysis, called exploratory confirmatory factor analysis (EFA CFA). EFA CFA aim reproduce observed relationships among group features smaller set latent variables. EFA used descriptive, data-driven manner uncover measured variables reasonable indicators various latent dimensions. contrast, CFA conducted -priori, hypothesis-testing manner requires strong empirical theoretical foundations. discuss EFA , used group features specified number latent factors.","code":""},{"path":"exploratory-factor-analysis.html","id":"practical-example-1","chapter":"4 Exploratory Factor Analysis","heading":"4.1 PRACTICAL EXAMPLE","text":"practical example section, work dataset containing marks 100 students various subjects, including English, Hindi, Maths, French, Physics, Poetry, Statistics, Geometry.can View download datasets ","code":""},{"path":"exploratory-factor-analysis.html","id":"analysis-1","chapter":"4 Exploratory Factor Analysis","heading":"4.2 Analysis","text":"First prepare data set save csv file. import data set R. See chapter 2to know save csv file import R. Also can directly use code import dataset computer R.Please note path copied system format C:\\Users\\HP\\Documents\\, change format C:/Users/HP/Documents/ R.using pysch package factor analysis.Install required packages:","code":"\ndata <- read.csv(\"path to your file\", row.names=1)\n#your file will be saved in the name data\ninstall.packages(\"psych\")  "},{"path":"exploratory-factor-analysis.html","id":"determining-the-suitability-of-your-dataset-for-efa","chapter":"4 Exploratory Factor Analysis","heading":"4.2.1 Determining the suitability of your dataset for EFA","text":"perform Kaiser–Meyer–Olkin (KMO) test (Kaiser 1970) & (Kaiser Rice 1974) test suitability dataset factor analysis.\nKaiser–Meyer–Olkin (KMO) test statistical measure determine suited data factor analysis. test measures sampling adequacy variable model complete model. Measure Sampling Adequacy (MSA) measure proportion variance among variables might common variance. higher proportion, higher KMO-value, suited data factor analysis.KMO values 0.8 1 indicate adequate sampling data suitable factor analysisKMO values 0.6 suggest inadequate sampling may require remedial action.KMO value close zero indicates presence large partial correlations relative sum correlations, can pose challenge factor analysis.\nperform KMO analysis using library psych R. Use code .example can see Kaiser-Meyer-Olkin (KMO) measure sampling adequacy (MSA), calculated dataset, indicates overall adequacy 0.84, suggesting data suitable factor analysis. Additionally, individual item-specific KMO values subject, including English, Hindi, Maths, French, Physics, Poetry, Statistics, Geometry, range 0.81 0.86, supporting adequacy data factor analysis.\ncase item-specific MSA < 0.60, better avoid item analysis.","code":"\nlibrary(psych)\n#import your csv and save it in the name \"data\" and use rownames as first column.\nKMO(data)## Kaiser-Meyer-Olkin factor adequacy\n## Call: KMO(r = data)\n## Overall MSA =  0.84\n## MSA for each item = \n##    English      Hindi      Maths     French    Physics     Poetry Statistics \n##       0.84       0.84       0.84       0.84       0.85       0.81       0.86 \n##   Geometry \n##       0.85"},{"path":"exploratory-factor-analysis.html","id":"identifying-the-number-of-factors","chapter":"4 Exploratory Factor Analysis","heading":"4.2.2 Identifying the number of factors","text":"discuss two methods identify number factors. methods can employed.","code":""},{"path":"exploratory-factor-analysis.html","id":"scree-plot-1","chapter":"4 Exploratory Factor Analysis","heading":"4.2.2.1 Scree plot","text":"Scree plot graphical representation used factor analysis identify optimal number factors retain dataset. displays eigenvalues correlation matrix, represent amount variance explained factor. employ Scree plot determine number factors retained given dataset. examining pattern eigenvalues Scree plot, can identify point eigenvalues significantly drop. drop-serves indicator meaningful influential factors, ensuring capture essential dimensions within data.","code":"\nev <- eigen(cor(data)) # get eigenvalues\nev$values## [1] 3.4434243 3.2878668 0.3070565 0.2643156 0.2072886 0.1817183 0.1653205\n## [8] 0.1430094\nscree(data, pc=FALSE)"},{"path":"exploratory-factor-analysis.html","id":"parallel-analysis","chapter":"4 Exploratory Factor Analysis","heading":"4.2.2.2 Parallel analysis","text":"Parallel analysis robust statistical technique employed factor analysis address crucial question many factors retained given dataset. method particularly valuable dealing complex data structures. Rather relying solely conventional rules subjective judgment, parallel analysis offers data-driven approach. involves generating random datasets similar properties original data comparing factor structures. scrutinizing eigenvalues derived random datasets alongside actual data, parallel analysis provides objective reliable means determining optimal number factors retain. approach enhances accuracy factor analysis, ensuring identified factors capture meaningful interpretable dimensions within dataset.Identify point eigenvalues actual data surpass corresponding eigenvalues random data significant margin. indicates number factors retained. example can see number identified factors two.","code":"\nlibrary(psych)\nparallel<-fa.parallel(data, fa=\"fa\")## Parallel analysis suggests that the number of factors =  2  and the number of components =  NA"},{"path":"exploratory-factor-analysis.html","id":"factor-analysis","chapter":"4 Exploratory Factor Analysis","heading":"4.2.3 Factor Analysis","text":"using built-function R factanal() perform factor analysis.","code":"\nNfacs <- 2  # Number of factors as identified in parallel analysis/Scree plot.\nfit <- factanal(data, Nfacs,rotation=\"varimax\",scores=\"regression\")"},{"path":"exploratory-factor-analysis.html","id":"factor-rotation","chapter":"4 Exploratory Factor Analysis","heading":"4.2.3.1 Factor rotation","text":"Rotation context factor analysis mathematical technique used simplify interpret results analysis. Rotation methods applied factor loadings, coefficients indicate strongly observed variable associated latent factor. loadings can challenging interpret factor analysis yields complex, overlapping, unclear results.\nRotation serves following primary purposes:• Simplification\n• Enhanced InterpretabilityTwo type rotation , Orthogonal Oblique. orthogonal rotation, factors forced uncorrelated , simplifying factor structure. oblique rotation, factors can correlated, allowing realistic interpretations factors expected related.R offers two rotation methods, include Varimax (orthogonal), Promax (oblique. Researchers choose appropriate rotation method based hypotheses data goals simplifying interpreting results factor analysis.R rotation option factanal() function may changed “promax” “varimax”. example used Varimax rotation, Promax Varimax rotation case giving results factor analysis, suggests underlying factor structure data may sufficiently simple well-defined. factor solution clear distinct, ’s possible different rotation methods converge solution.","code":""},{"path":"exploratory-factor-analysis.html","id":"factor-loadings","chapter":"4 Exploratory Factor Analysis","heading":"4.2.3.2 Factor loadings","text":"Factor loadings coefficients represent relationships observed variables (indicators) latent factors factor analysis. coefficients indicate strength direction associations variable factor. Factor loadings play crucial role understanding underlying structure data extracting meaningful information factor analysis.’s interpret factor loadings:Magnitude Loadings: magnitude loading represents strength relationship variable factor. Larger loading values indicate stronger association. example, loading 0.7 stronger loading 0.3.Sign Loadings: sign loading (positive negative) indicates direction relationship. positive loading suggests increase variable associated increase factor, negative loading suggests inverse relationship.Interpretation: Interpret factor loadings considering variables high (absolute) loadings factor. Variables high loadings particular factor strongly associated factor contribute significantly .Cross-Loadings: variables may moderate loadings multiple factors. called cross-loadings suggest variable related one factor. Cross-loadings can provide insights complexity data.Loadings Close 0: Loadings close 0 indicate weak negligible relationship variable factor. Variables loadings near 0 contribute significantly factor can often removed analysis simplify factor structure.Naming Factors: Assign meaningful labels factors based variables high loadings. variables load strongly factor can help identify underlying theme concept factor represents.Lets find loadings exampleIn output, factor loadings factor analysis two factors, referred “Factor1” “Factor2.” Factor1 Factor 2 together explains 78.9% variance, can see Cumulative Var result. Proportion Var result gives proportion variance explained factor. loadings represent relationships observed variables latent factors. ’s interpretation loadings:“Factor1”:\n- variables “Maths,” “Physics,” “Statistics,” “Geometry” high positive loadings, values around 0.88 0.91. indicates strongly associated “Factor1.”“Factor1” seems represent set variables related quantitative analytical subjects, given strong loadings mathematics-related subjects “Maths,” “Physics,” “Statistics.” Factor 1 can named analytical ability.“Factor2”:\n- variables “English,” “Hindi,” “French,” “Poetry” high positive loadings, around 0.85 0.91, “Factor2.” loadings suggest strong association “Factor2.”“Factor2” appears capture different set variables, likely related language literature, given strong loadings languages “Poetry.”interpretation suggests factors distinct represent different aspects data. “Factor1” associated quantitative subjects, “Factor2” associated language literature-related variables. Factor 2 can named “Linguistic aptitude”.can see loadings blank. fact, loadings smaller 0.1 absolute value omitted (sparse structure displayed clearly).","code":"\nfit$loadings## \n## Loadings:\n##            Factor1 Factor2\n## English             0.877 \n## Hindi               0.879 \n## Maths       0.909         \n## French              0.849 \n## Physics     0.901         \n## Poetry              0.910 \n## Statistics  0.894         \n## Geometry    0.879         \n## \n##                Factor1 Factor2\n## SS loadings      3.217   3.092\n## Proportion Var   0.402   0.387\n## Cumulative Var   0.402   0.789"},{"path":"exploratory-factor-analysis.html","id":"communality","chapter":"4 Exploratory Factor Analysis","heading":"4.2.3.3 Communality","text":"Communality represents proportion variance observed variable (indicator) accounted underlying factors. words, communality measures much variability variable explained common factors extracted factor analysis. Communality value 0 1, 0 indicates none variance variable explained common factors (.e., unique variable ), 1 suggests variance explained common factors.Interpretation: higher communality value variable indicates larger proportion variance can attributed underlying factors. Variables high communality values well-represented factors strong associations underlying dimensions.\nCommunality can also calculated sum squared factor loadings variable. words, quantifies much variability variable can attributed factors, evidenced squares factor loadings.\nuse Rcode find communality.results example, high communality values, ranging approximately 0.720 0.834, indicate substantial portion variability variables can attributed underlying factors. suggests factors effective capturing essence variables, making suitable representing latent dimensions uncovered factor analysis. High communality values reflect strong association variables common factors, contributing straightforward interpretable factor structure.","code":"\napply(fit$loadings^2,1,sum)##    English      Hindi      Maths     French    Physics     Poetry Statistics \n##  0.7695653  0.7729351  0.8263179  0.7207453  0.8125261  0.8340212  0.7992973 \n##   Geometry \n##  0.7739642"},{"path":"exploratory-factor-analysis.html","id":"uniqueness","chapter":"4 Exploratory Factor Analysis","heading":"4.2.3.4 Uniqueness","text":"uniqueness (also known “specific variance” “unique variance”) concept represents variance observed variable explained latent variables model. Uniqueness value typically 0 1, 0 indicates variance variable explained common factors, 1 implies none variance accounted factors.Interpretation: low uniqueness value suggests variable highly related underlying factors model. case, variance variable explained common factors, indicating variable good indicator latent constructs.uniqueness values variables obtained reveal proportion variance specific variable shared common factors. instance, high uniqueness values, like approximately 27.93% “French,” indicate substantial part variance “French” unique language, highlighting distinct characteristics within factor analysis model. uniqueness values assist researchers determining extent variables idiosyncratic can essential identifying variables may fit well factor structure gaining insights individual features variable. Overall low values uniqueness indicate significant portion variance variables accounted latent factors, can positive sign quality factor analysis.can also calculate correlation variables using Rcode , insights required.","code":"\nfit$uniquenesses##    English      Hindi      Maths     French    Physics     Poetry Statistics \n##  0.2304343  0.2270650  0.1736824  0.2792547  0.1874740  0.1659783  0.2007031 \n##   Geometry \n##  0.2260363\ncorrel <- round(fit$correlation, 3) # round correlation to 3 digits\ncorrel##            English  Hindi  Maths French Physics Poetry Statistics Geometry\n## English      1.000  0.744  0.008  0.786  -0.027  0.794      0.010    0.041\n## Hindi        0.744  1.000 -0.017  0.740  -0.035  0.824     -0.029   -0.009\n## Maths        0.008 -0.017  1.000  0.003   0.826 -0.074      0.795    0.810\n## French       0.786  0.740  0.003  1.000   0.003  0.749      0.038   -0.004\n## Physics     -0.027 -0.035  0.826  0.003   1.000 -0.080      0.815    0.772\n## Poetry       0.794  0.824 -0.074  0.749  -0.080  1.000     -0.061   -0.015\n## Statistics   0.010 -0.029  0.795  0.038   0.815 -0.061      1.000    0.796\n## Geometry     0.041 -0.009  0.810 -0.004   0.772 -0.015      0.796    1.000"},{"path":"exploratory-factor-analysis.html","id":"draw-diagrams","chapter":"4 Exploratory Factor Analysis","heading":"4.2.3.5 Draw diagrams","text":"Use R code draw diagram visualize factor analysis. Lets simply visualize variables correlation. code good visualize number variables .can save diagrams high resolution using codes :\nNow draw factor diagram using code :","code":"\n# install.packages(\"PerformanceAnalytics\")\nlibrary(\"PerformanceAnalytics\")\nchart<-chart.Correlation(data, histogram=TRUE, pch=19)\nloads <- fit$loadings\nfa_diag<-fa.diagram(loads)\n#saving visulaization plot \npng(\"Plot.png\", res = 72, width = 1000, height = 1000, pointsize = 25)\nPerformanceAnalytics::chart.Correlation(data, histogram=TRUE, pch=19)\ndev.off()  \n\n#saving factor analysis diagram\npng(\"Plot.png\", res = 72, width = 1000, height = 1000, pointsize = 25)\nloads <- fit$loadings\nfa_diag<-fa.diagram(loads)\ndev.off()  \n\n## change to required resolution in the parameter res ="},{"path":"cluster-analysis.html","id":"cluster-analysis","chapter":"5 Cluster Analysis","heading":"5 Cluster Analysis","text":"Cluster analysis, also known clustering, data analysis technique used statistics, machine learning, data mining. involves process grouping set objects data points clusters subsets, aim organizing similar items together keeping dissimilar items apart. primary goal cluster analysis identify patterns, structures, natural groupings within dataset.(Hastie, Tibshirani, Friedman 2009)\nMainly two types clustering discussed section.Hierarchical clusteringHierarchical clusteringK-means clusteringK-means clusteringWe using cluster, ggplot2, pheatmap, NbClust,factoextra factominer packages cluster analysis.Install required packages:using “mtcars” dataset, well-known dataset R contains information various car models 1970s, specifically 1973 1974. provides data characteristics cars, miles per gallon (mpg), number cylinders, horsepower, weight, .\nperform cluster analysis using data.","code":"\n# install required pacakges\ninstall.packages(c(\"cluster\", \"factominer\",\"ggplot2\",\"pheatmap\"))  "},{"path":"cluster-analysis.html","id":"importcsv","chapter":"5 Cluster Analysis","heading":"5.1 Dataset import","text":"First prepare data set similar save csv file. import data set R. See chapter 2to know save csv file import R. Also can directly use code import dataset computer R.Please note path copied system format C:\\Users\\HP\\Documents\\, change format C:/Users/HP/Documents/ R.","code":"\ndata <- read.csv(\"path to your file\", row.names=1)\n#your file will be saved in the name data"},{"path":"cluster-analysis.html","id":"hierarchical-clustering","chapter":"5 Cluster Analysis","heading":"5.2 Hierarchical clustering","text":"unsupervised clustering method groups data points clusters based similarities without using predefined labels categories. often used exploratory data analysis visualization.\nHierarchical clustering used don’t prior knowledge number clusters data points grouped. starts data point cluster gradually merges based similarities.Hierarchical clustering widely used biology gene expression analysis, marketing customer segmentation, various fields taxonomy, objective discover natural hierarchy groupings data.Two approaches :","code":""},{"path":"cluster-analysis.html","id":"agglomerative-clustering","chapter":"5 Cluster Analysis","heading":"5.2.1 Agglomerative clustering","text":"agglomerative approach, data points start individual clusters successively merged","code":""},{"path":"cluster-analysis.html","id":"divisive-clustering","chapter":"5 Cluster Analysis","heading":"5.2.2 Divisive clustering","text":"divisive data points begin single cluster successively split smaller clusters.","code":""},{"path":"cluster-analysis.html","id":"dendrogram","chapter":"5 Cluster Analysis","heading":"5.2.3 Dendrogram","text":"Hierarchical clustering produces dendrogram, tree-like diagram illustrates merging splitting clusters different levels. dendrogram helps visualize hierarchy clusters can guide selection optimal number clusters.","code":""},{"path":"cluster-analysis.html","id":"hierrachial-clustering-in-r","chapter":"5 Cluster Analysis","heading":"5.3 Hierrachial clustering in R","text":"","code":""},{"path":"cluster-analysis.html","id":"agglomerative-hierrarchial-clustering","chapter":"5 Cluster Analysis","heading":"5.3.1 Agglomerative hierrarchial clustering","text":"First step normalize data, sometimes variables different scales, need normalized based scale function clustering data sets.R function hclust() used. takes dissimilarity matrix input, calculated using function dist().\nCompute distance matrix use hclust() function appropriate linkage method.","code":"\n#import your csv and save it in the name \"data\" and use row names as first column.\ndata <- scale(data)\n# perform agglomerative clustering with linkage method ward.D2\nlibrary(\"factoextra\")\nres.hc <- hclust(dist(data),  method = \"ward.D2\")\n# create dendrogram\np<-factoextra::fviz_dend(res.hc, cex = 0.5)\n# cex: label size  \np"},{"path":"cluster-analysis.html","id":"linkage-methods","chapter":"5 Cluster Analysis","heading":"5.3.2 Linkage methods","text":"Several linkage methods commonly used hierarchical clustering. ’s brief summary methods:\ncan use method inside argument method hclust() function“ward.D” “ward.D2”: Ward’s method minimizes increase within-cluster sum squares merging clusters. “ward.D” uses Euclidean distance, “ward.D2” uses squared Euclidean distance. methods often preferred want create well-balanced clusters.“single”: Single linkage calculates minimum pairwise distance two points different clusters. tends create stringy, elongated clusters can sensitive outliers.“complete”: Complete linkage calculates maximum pairwise distance points different clusters. produces compact clusters less sensitive outliers single linkage.“average”: Average linkage computes average distance pairs data points different clusters. balances trade-single complete linkage common choice many applications.“mcquitty”: McQuitty linkage modification complete linkage designed address issues associated complete linkage. weighted method reduces sensitivity outliers.“median”: Median linkage calculates distance medians clusters. can result clusters various shapes sizes.“centroid”: Centroid linkage calculates distance centroids clusters. Similar median linkage, can lead clusters different shapes sizes.linkage methods allow control clusters formed hierarchical clustering process. choice method depends specific data, objectives, preferences characteristics resulting clusters. Experimenting different methods assessing suitability particular dataset often necessary determine appropriate linkage method. example used ward.D2.","code":""},{"path":"cluster-analysis.html","id":"customizing-dendrogram","chapter":"5 Cluster Analysis","heading":"5.3.3 Customizing dendrogram","text":"Suppose decide 7 clusters want colour accordingly use code:can save dendrogram using code :","code":"\np<- fviz_dend(res.hc, cex = 0.5, k = 7, palette = \"jco\") \n# 'k=' specifies the number of clusters\np\np<-factoextra::fviz_dend(res.hc, cex = 0.5, k=7, rect = TRUE)\np\n# Save dendrogram\nlibrary(ggplot2)\nggsave (\"dendro.png\",plot = p,width=6, height = 4, dpi=300)"},{"path":"cluster-analysis.html","id":"heat-map","chapter":"5 Cluster Analysis","heading":"5.3.3.1 Heat Map","text":"heatmap another way visualize hierarchical clustering. ’s also called false colored image, data values transformed color scale. Heat maps allow us simultaneously visualize groups samples features. can easily create pretty heatmap using R package pheatmap.can save heatmap using code :heatmap, generally, columns samples rows variables.","code":"\nlibrary(pheatmap)\nheatmap<- pheatmap(t(data), cutree_cols = 7)  \nheatmap\n# Save heat map\nggsave (\"heatmap.png\",plot = heatmap,width=6, height = 4, dpi=300)"},{"path":"cluster-analysis.html","id":"silhouette-plot","chapter":"5 Cluster Analysis","heading":"5.3.3.2 silhouette plot","text":"silhouette score widely used metric evaluating quality clusters unsupervised machine learning. provides measure well-separated distinct clusters , offering valuable insights effectiveness clustering algorithm. silhouette score calculated data point, assessing similarity data points within cluster compared nearest neighboring cluster. ranges -1 1, high silhouette score (close 1) indicates data point appropriately placed cluster, low score (close -1) suggests might suited different cluster. silhouette score close zero implies data point close decision boundary two clusters. metric aids selecting optimal number clusters valuable tool cluster validation interpretation.\nsilhouette plot y-axis typically represents individual data points samples x-axis represents silhouette values. data point dataset represented plot vertical bar. height bar corresponds silhouette value data point.\nSilhouette plots organized clusters, cluster’s data points grouped together. helps visualize structure individual clusters overall quality.","code":"\n# Silhoutte plot  \nlibrary(cluster)\ndistance <-dist(data)\nplot(silhouette(cutree(res.hc,7), distance))"},{"path":"cluster-analysis.html","id":"divisive-hierrarchial-clustering","chapter":"5 Cluster Analysis","heading":"5.3.4 Divisive hierrarchial clustering","text":"Also known DIvisive ANAlysis (DIANA) Clustering. lecture limiting discussion Agglomerative clustering alone divisive clustering much widely used agricultural research.","code":""},{"path":"cluster-analysis.html","id":"k-means-clustering","chapter":"5 Cluster Analysis","heading":"5.4 K means clustering","text":"K means clustering another unsupervised clustering method partitions data points k clusters based similarities. widely used grouping data number clusters known can estimated.Unlike hierarchical clustering, k-means requires specify number clusters (k) advance. tries find cluster centroids assigns data points nearest centroid based features. K-means centroid-based algorithm, centroids represent center cluster. Data points assigned cluster whose centroid closest .K-means used image compression, customer segmentation, document clustering, recommendation systems. versatile method grouping data number clusters known can estimated based problem domain.summary, hierarchical clustering method discovering natural hierarchy clusters data, can applied unsupervised scenarios number clusters predetermined. hand, k-means clustering centroid-based method used partition data specified number clusters, making suitable unsupervised clustering tasks known estimated number clusters.","code":""},{"path":"cluster-analysis.html","id":"k-means-using-r","chapter":"5 Cluster Analysis","heading":"5.4.1 K means using R","text":"using usarrests dataset discussed previous sections perform K means clustering.First step normalize data, sometimes variables different scales, need normalized based scale function clustering data sets.","code":"\n#import your csv and save it in the name \"data\" and use row names as first column.\ndata <- scale(data)"},{"path":"cluster-analysis.html","id":"finding-optimum-number-of-clusters","chapter":"5 Cluster Analysis","heading":"5.4.2 Finding optimum number of clusters","text":"","code":""},{"path":"cluster-analysis.html","id":"elbow-method","chapter":"5 Cluster Analysis","heading":"5.4.2.1 Elbow method","text":"use Elbow method, common technique used determine optimal number clusters K-means clustering. involves running K-means clustering dataset range values K (number clusters) plotting sum squared distances point assigned cluster center function K. “elbow” point plot typically optimal number clusters, represents point adding clusters significantly reduce sum squared distances.Save plot required dpi using code ","code":"\nlibrary(factoextra)\nlibrary(NbClust)\n# Elbow method\nP1<-fviz_nbclust(data, kmeans, method = \"wss\") +\n  geom_vline(xintercept = 4, linetype = 2)+\n  labs(subtitle = \"Elbow method\")\nP1\nlibrary(ggplot2)\nggsave(P1, file=\"optimCluster.png\", width=10, height=5, dpi = 700)"},{"path":"cluster-analysis.html","id":"gap-statistic-method","chapter":"5 Cluster Analysis","heading":"5.4.2.2 Gap Statistic method","text":"gap statistic method helps us find optimal number clusters dataset. comparing variation within clusters different values “k” (number clusters) expect chance. best number clusters one gives us largest gap statistic. words, tells us data’s clustering structure distinct random distribution points.(Tibshirani, Walther, Hastie 2001)","code":"\nfviz_nbclust(data, kmeans, nstart = 25,  method = \"gap_stat\", nboot = 50)+\n  labs(subtitle = \"Gap statistic method\")"},{"path":"cluster-analysis.html","id":"silhouette-method","chapter":"5 Cluster Analysis","heading":"5.4.2.3 Silhouette method","text":"average silhouette approach method used evaluate quality clustering. assesses well data point fits within assigned cluster. high average silhouette width indicates good clustering, suggesting data points well-separated within clusters clustering meaningful well-defined.methods indicating different number clusters.elbow average silhouette methods limitation – give general sense good clustering , don’t provide precise number clusters.\nadvanced approach gap statistic. can give reliable estimate ideal number clusters data.\nexample moving number clusters 3, (k=3).","code":"\n# Silhouette method \nset.seed(123)\nfviz_nbclust(data, kmeans, method = \"silhouette\")+\n  labs(subtitle = \"Silhouette method\")"},{"path":"cluster-analysis.html","id":"computing-k-means","chapter":"5 Cluster Analysis","heading":"5.4.3 Computing K means","text":"can see cluster sizes","code":"\n# Compute k-means with k = 3\nset.seed(123)\nkm.res <- kmeans(data, 3, nstart = 25)\n# Cluster size\nkm.res$size## [1] 20 13 17\n#Cluster means\nkm.res$centers  ##       Murder    Assault   UrbanPop       Rape\n## 1  1.0049340  1.0138274  0.1975853  0.8469650\n## 2 -0.9615407 -1.1066010 -0.9301069 -0.9667633\n## 3 -0.4469795 -0.3465138  0.4788049 -0.2571398"},{"path":"cluster-analysis.html","id":"plotting-k-means","chapter":"5 Cluster Analysis","heading":"5.4.4 Plotting K means","text":"can plot K means clustering using code .Save plot using code ","code":"\nplot_k<-fviz_cluster(km.res, data,\n             palette = \"Set1\", ggtheme = theme_minimal())\nplot_k\nggsave(plot_k, file=\"plot_k.png\", width=10, height=5, dpi = 700)"},{"path":"logistic-regression.html","id":"logistic-regression","chapter":"6 Logistic Regression","heading":"6 Logistic Regression","text":"Logistic regression statistical method used predicting category class individuals based one multiple predictor variables (denoted “x”). specifically designed modeling binary outcome, response variable can take one two possible values, 0 1, yes , diseased non-diseased.modeling technique part Generalized Linear Model (GLM) family, extends principles linear regression handle broader range data scenarios. Logistic regression also known various names, including binary logistic regression, binomial logistic regression, logit model.’s essential note logistic regression doesn’t directly provide class labels observations. Instead, estimates probability (denoted “p”) observation belonging particular class. probability value falls within range 0 1. determine class assignment, must select threshold probability. default, threshold often set p = 0.5, practice, choice threshold determined based specific goals analysis.\nstandard logistic regression function represented :\\[\np = \\frac{e^{y}}{1 + e^{y}}\n\\]\nsimply:\n\\[\np = \\frac{1}{1 + e^{-y}}\n\\]\n:\\(y = b_0 + b_1 \\cdot x\\)\\(y = b_0 + b_1 \\cdot x\\)\\(e\\) represents exponential function\\(e\\) represents exponential function\\(p\\) represents probability event occurring given \\(x\\), denoted \\(p(\\text{{event}}=1|x)\\) abbreviated \\(p(x)\\), \\[\np(x) = \\frac{1}{1 + e^{-(b_0 + b_1 \\cdot x)}}\n\\]\nbit manipulation, can demonstrated :\n\\[\n\\frac{p}{1-p} = e^{b_0 + b_1 \\cdot x}\n\\]\\(p\\) represents probability event occurring given \\(x\\), denoted \\(p(\\text{{event}}=1|x)\\) abbreviated \\(p(x)\\), \\[\np(x) = \\frac{1}{1 + e^{-(b_0 + b_1 \\cdot x)}}\n\\]\nbit manipulation, can demonstrated :\n\\[\n\\frac{p}{1-p} = e^{b_0 + b_1 \\cdot x}\n\\]Taking logarithm sides, get formula linear combination predictors:\\[\n\\log\\left(\\frac{p}{1-p}\\right) = b_0 + b_1 \\cdot x\n\\]dealing multiple predictor variables, logistic function takes form:\\[\n\\log\\left(\\frac{p}{1-p}\\right) = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + \\ldots + b_n \\cdot x_n\n\\], \\(b_0\\) \\(b_1\\) regression beta coefficients. positive \\(b_1\\) indicates increase \\(x\\) associated increase \\(p\\), negative \\(b_1\\) indicates increase \\(x\\) associated decrease \\(p\\).term \\(\\log\\left(\\frac{p}{1-p}\\right)\\) known logarithm odds, often referred log-odds logit. odds represent likelihood event occurring can thought ratio “successes” “non-successes”. Technically, odds calculated probability event divided probability event occur. instance, probability diabetes-positive 0.5, probability “positive” \\(1 - 0.5 = 0.5\\), odds 1.0.\ncan calculate probability odds using formula:\n\\[  \np = \\frac{\\text{Odds}}{1 + \\text{Odds}}\n\\]","code":""},{"path":"logistic-regression.html","id":"practical-example-2","chapter":"6 Logistic Regression","heading":"6.1 Practical Example","text":"proceeding sections install following packages: texreg, ggplot2,dplyrWe using part PimaIndiansDiabetes2 dataset mlbench package. can download view model dataset .","code":"\n# install required packages\ninstall.packages(c(\"texreg\", \"ggplot2\",\"dplyr\"))  "},{"path":"logistic-regression.html","id":"dataset-import","chapter":"6 Logistic Regression","heading":"6.2 Dataset import","text":"First prepare data set similar save csv file. import data set R. See chapter 2to know save csv file import R. Also can directly use code import dataset computer R.Please note path copied system format C:\\Users\\HP\\Documents\\, change format C:/Users/HP/Documents/ R.Now fit model","code":"\ndata <- read.csv(\"path to your file\", row.names=1)\n#your file will be saved in the name data\n# Fit the model\nmodel <- glm( as.factor(diabetes) ~., data = data, family = binomial)\n# Summarize the model\nsummary(model)  ## \n## Call:\n## glm(formula = as.factor(diabetes) ~ ., family = binomial, data = data)\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.004e+01  1.218e+00  -8.246  < 2e-16 ***\n## pregnant     8.216e-02  5.543e-02   1.482  0.13825    \n## glucose      3.827e-02  5.768e-03   6.635 3.24e-11 ***\n## pressure    -1.420e-03  1.183e-02  -0.120  0.90446    \n## triceps      1.122e-02  1.708e-02   0.657  0.51128    \n## insulin     -8.253e-04  1.306e-03  -0.632  0.52757    \n## mass         7.054e-02  2.734e-02   2.580  0.00989 ** \n## pedigree     1.141e+00  4.274e-01   2.669  0.00760 ** \n## age          3.395e-02  1.838e-02   1.847  0.06474 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 498.10  on 391  degrees of freedom\n## Residual deviance: 344.02  on 383  degrees of freedom\n## AIC: 362.02\n## \n## Number of Fisher Scoring iterations: 5"},{"path":"logistic-regression.html","id":"viewing-results","chapter":"6 Logistic Regression","heading":"6.2.1 Viewing results","text":"Viewing results better way.","code":"\nlibrary(texreg)\nscreenreg(model)  ## \n## ===========================\n##                 Model 1    \n## ---------------------------\n## (Intercept)      -10.04 ***\n##                   (1.22)   \n## pregnant           0.08    \n##                   (0.06)   \n## glucose            0.04 ***\n##                   (0.01)   \n## pressure          -0.00    \n##                   (0.01)   \n## triceps            0.01    \n##                   (0.02)   \n## insulin           -0.00    \n##                   (0.00)   \n## mass               0.07 ** \n##                   (0.03)   \n## pedigree           1.14 ** \n##                   (0.43)   \n## age                0.03    \n##                   (0.02)   \n## ---------------------------\n## AIC              362.02    \n## BIC              397.76    \n## Log Likelihood  -172.01    \n## Deviance         344.02    \n## Num. obs.        392       \n## ===========================\n## *** p < 0.001; ** p < 0.01; * p < 0.05"},{"path":"logistic-regression.html","id":"interpretation","chapter":"6 Logistic Regression","heading":"6.2.2 Interpretation","text":"analysis logistic regression model reveals eight predictors considered, three show statistically significant associations outcome variable. significant predictors glucose, pedigree mass.\nlogistic regression calculated, regression coefficient (b1) estimated increase log odds outcome per unit increase value variable (x1). variable “glucose,” positive coefficient estimate b = 0.04, increase glucose associated increased probability diabetes-positive. means higher glucose levels tend indicate higher likelihood diabetes.","code":""},{"path":"logistic-regression.html","id":"odds-ratio-or","chapter":"6 Logistic Regression","heading":"6.2.3 Odds ratio (OR)","text":"represents odds outcome occur presence particular variable, compared odds outcome occurring absence particular variable.=1 Variable affect odds outcomeOR=1 Variable affect odds outcomeOR>1 Variable associated higher odds outcomeOR>1 Variable associated higher odds outcomeOR<1 Variable associated lower odds outcome\nOdds ratio calculated exponential function regression coefficient \\(e^{b_1}\\).<1 Variable associated lower odds outcome\nOdds ratio calculated exponential function regression coefficient \\(e^{b_1}\\).instance, regression coefficient “glucose” 0.04 indicates one-unit increase glucose concentration increases odds diabetes-positive factor exp(0.042), approximately 1.04 times.","code":""},{"path":"logistic-regression.html","id":"model-comparisons","chapter":"6 Logistic Regression","heading":"6.2.4 Model comparisons","text":"","code":""},{"path":"logistic-regression.html","id":"akaike-information-criteria-aic","chapter":"6 Logistic Regression","heading":"6.2.4.1 Akaike Information Criteria (AIC)","text":"AIC measure goodness fit statistical model. commonly used model selection comparing several models determine one best fit given dataset. Lower AIC values indicate better balance model fit simplicity.","code":""},{"path":"logistic-regression.html","id":"bayesian-information-criterion-bic","chapter":"6 Logistic Regression","heading":"6.2.4.2 Bayesian Information Criterion (BIC)","text":"BIC similar measure AIC places stronger penalty complex models. used model selection, much like AIC. BIC considers model fit model complexity often preferred want prevent overfitting select simpler model.","code":""},{"path":"logistic-regression.html","id":"log-likelihood","chapter":"6 Logistic Regression","heading":"6.2.4.3 log-likelihood","text":"log-likelihood fundamental component AIC BIC. represents logarithm likelihood data given model, measures well model explains observed data. often used compare fit different models, higher log-likelihood indicates better fit data.","code":""},{"path":"logistic-regression.html","id":"deviance-measures","chapter":"6 Logistic Regression","heading":"6.2.4.4 Deviance measures","text":"Null deviance : measure model’s goodness fit intercept (predictor variables) included model. represents deviance model contains explanatory variables essentially null model.\nexample, null deviance 498.10, calculated 391 degrees freedom.\nResidual Deviance:\nresidual deviance measure model’s goodness fit predictor variables included. quantifies deviance model observed data adjusting parameters model. example, residual deviance 344.02, calculated 383 degrees freedom.","code":""},{"path":"logistic-regression.html","id":"fitting-a-better-model","chapter":"6 Logistic Regression","heading":"6.3 Fitting a better model","text":"logistic regression results, can noticed variables - pregnent, triceps, insulin age - statistically significant. Keeping model may contribute overfitting. Therefore, eliminated. better model can fitted .new model can see AIC reduced 356.89 compared previous model AIC 362.","code":"\nmodel <- glm( as.factor(diabetes) ~ age+glucose + mass + pedigree+pregnant, data = data, family = binomial)\n# Summarize the model\nsummary(model)  ## \n## Call:\n## glm(formula = as.factor(diabetes) ~ age + glucose + mass + pedigree + \n##     pregnant, family = binomial, data = data)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -9.992080   1.086866  -9.193  < 2e-16 ***\n## age          0.034360   0.017810   1.929 0.053692 .  \n## glucose      0.036458   0.004978   7.324 2.41e-13 ***\n## mass         0.078139   0.020605   3.792 0.000149 ***\n## pedigree     1.150913   0.424242   2.713 0.006670 ** \n## pregnant     0.083953   0.055031   1.526 0.127117    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 498.10  on 391  degrees of freedom\n## Residual deviance: 344.89  on 386  degrees of freedom\n## AIC: 356.89\n## \n## Number of Fisher Scoring iterations: 5"},{"path":"logistic-regression.html","id":"saving-results-to-csv","chapter":"6 Logistic Regression","heading":"6.4 Saving results to csv","text":"can use code save results csv.","code":"\n# Extract coefficients, standard errors, and p-values\ncoef <- as.data.frame(round(model$coefficients[, c(\"Estimate\", \"Std. Error\", \"Pr(>|z|)\")], 3))\n# Rename the columns for clarity\ncolnames(coef) <- c(\"Coefficient\", \"Std.Error\", \"P.Value\")\n# write to csv\nwrite.csv(coef, \"result_logit.csv\")"},{"path":"logistic-regression.html","id":"plotting-logistic-curve","chapter":"6 Logistic Regression","heading":"6.5 Plotting logistic curve","text":"can draw logistic curve single variable models like using code.","code":"\nlibrary(ggplot2)  \nlibrary(dplyr)\ndata %>%\n  mutate(prob = ifelse(diabetes == \"pos\", 1, 0)) %>%\n  ggplot(aes(glucose, prob)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(\n    title = \"Logistic Regression Model\", \n    x = \"Glucose\",\n    y = \"Probability of being diabetic\"\n    )+theme_bw()"},{"path":"canonical-correlation.html","id":"canonical-correlation","chapter":"7 Canonical Correlation","heading":"7 Canonical Correlation","text":"Canonical correlation analysis (CCA) multivariate statistical technique explores relationships two sets variables (X Y) measured set individuals observations.Canonical correlation analysis first introduced American statistician psychologist Harold Hotelling early 1930s. Hotelling’s seminal work technique published article “Relations two sets variates” 1936.(Hotelling 1936)CCA used identify quantify associations two sets variables, providing insights related, aims find linear combinations variables (canonical variates) set maximally correlated .canonical variates designed capture significant relationships patterns two sets variables. linear combinations used understand quantify associations two sets.\n’s canonical variates typically defined:\nSet X:\\[U_1 = a_{11}X_1 + a_{12}X_2 + \\ldots + a_{1p}X_p\\]\\[U_2 = a_{21}X_1 + a_{22}X_2 + \\ldots + a_{2p}X_p\\]Set Y:\\[V_1 = b_{11}Y_1 + b_{12}Y_2 + \\ldots + b_{1q}Y_q\\]\\[V_2 = b_{21}Y_1 + b_{22}Y_2 + \\ldots + b_{2q}Y_q\\]equations, U₁ U₂ canonical variates derived Set X, V₁ V₂ canonical variates derived Set Y. coefficients (’s b’s) calculated correlation U₁ V₁ (first canonical pair) maximized, similarly U₂ V₂ (second canonical pair), . canonical variables within set orthogonal , meaning uncorrelated. orthogonality simplifies interpretation results.Canonical Correlation Analysis allows us summarize relationships fewer statistics preserving main facets relationships. way, motivation canonical correlation similar principal component analysis. another dimension-reduction technique.select X Y based number variables set \\(p \\leq q\\). p canonical correlations.","code":""},{"path":"canonical-correlation.html","id":"canonical-correlation-in-r","chapter":"7 Canonical Correlation","heading":"7.1 Canonical Correlation in R","text":"packages required analysis areCCA CCP.Install required packages:","code":"\n# install required pacakges\ninstall.packages(c(\"CCA\",\"CCP\",))  "},{"path":"canonical-correlation.html","id":"practical-examople","chapter":"7 Canonical Correlation","heading":"7.2 Practical Examople","text":"example data comes firm surveyed random sample n = 50 employees attempt determine factors influence sales performance. Two collections variables measured:Sales Performance:Sales GrowthSales GrowthSales ProfitabilitySales ProfitabilityNew Account SalesNew Account SalesTest Scores Measure Intelligence:CreativityCreativityMechanical ReasoningMechanical ReasoningAbstract ReasoningAbstract ReasoningMathematicsMathematicsThere p = 3 variables first group relating Sales Performance q = 4 variables second group relating Test Scores. 3 canonical correlations.can view download data .","code":""},{"path":"canonical-correlation.html","id":"dataset-import-1","chapter":"7 Canonical Correlation","heading":"7.3 Dataset import","text":"First prepare data set similar save csv file. import data set R. See chapter 2to know save csv file import R. Also can directly use code import dataset computer R.Please note path copied system format C:\\Users\\HP\\Documents\\, change format C:/Users/HP/Documents/ R.","code":"\ndata <- read.csv(\"path to your file\")\n#your file will be saved in the name data"},{"path":"canonical-correlation.html","id":"analysis-2","chapter":"7 Canonical Correlation","heading":"7.3.1 Analysis","text":"","code":"\n#Define X and Y in your data  \nattach(data)\nsp <- data[, c(\"growth\", \"profit\", \"new\")]\nts <- data[, c(\"create\", \"mech\", \"abs\", \"math\")]"},{"path":"canonical-correlation.html","id":"testing-hypothesis","chapter":"7 Canonical Correlation","heading":"7.3.1.1 Testing Hypothesis","text":"Testing null hypothesis p canonical variate pairs uncorrelated . Following hypothesis tested.\nH0: \\(\\rho_1 = \\rho_2 = \\rho_3 = 0\\)\nH0: \\(\\rho_2 = \\rho_3 = 0\\)\nH0: \\(rho_3 = 0\\)Wilks’ Lambda used test statistic assessing significance canonical correlations. extremely low p-value (essentially zero) suggests strong evidence reject three null hypothesis.three canonical variate pairs significantly correlated dependent one another. suggests may summarize three pairs. practice, tests carried successively find non-significant result. non-significant result found, stop. happens first canonical variate pair, sufficient evidence relationship two sets variables analysis may stop.first pair shows significance, move second canonical variate pair. second pair significantly correlated stop. significant continue third pair, proceeding iterative manner pairs canonical variates testing find non-significant results.","code":"\n# Calculate the number of rows (observations) in the 'sp' dataset and store it in 'n'.\nn <- dim(sp)[1]\n# Calculate the number of columns (variables) in the 'sp' dataset and store it in 'p'.\np <- length(sp)\n# Calculate the number of columns (variables) in the 'ts' dataset and store it in 'q'.\nq <- length(ts)\n#testing hypothesis\nlibrary(CCA)\nlibrary(CCP)\ncc1 <- cc(sp, ts)\nrho <- cc1$cor\np.asym(rho, n, p, q, tstat = \"Wilks\")## Wilks' Lambda, using F-approximation (Rao's F):\n##                 stat    approx df1      df2      p.value\n## 1 to 3:  0.002148472 87.391525  12 114.0588 0.000000e+00\n## 2 to 3:  0.195241267 18.526265   6  88.0000 8.248957e-14\n## 3 to 3:  0.852846693  3.882233   2  45.0000 2.783536e-02\n# correlations using the matcor function from the CCA package\nlibrary(CCA)\nlibrary(CCP)\nmatcor(sp, ts)## $Xcor\n##           growth    profit       new\n## growth 1.0000000 0.9260758 0.8840023\n## profit 0.9260758 1.0000000 0.8425232\n## new    0.8840023 0.8425232 1.0000000\n## \n## $Ycor\n##           create      mech       abs      math\n## create 1.0000000 0.5907360 0.1469074 0.4126395\n## mech   0.5907360 1.0000000 0.3859502 0.5745533\n## abs    0.1469074 0.3859502 1.0000000 0.5663721\n## math   0.4126395 0.5745533 0.5663721 1.0000000\n## \n## $XYcor\n##           growth    profit       new    create      mech       abs      math\n## growth 1.0000000 0.9260758 0.8840023 0.5720363 0.7080738 0.6744073 0.9273116\n## profit 0.9260758 1.0000000 0.8425232 0.5415080 0.7459097 0.4653880 0.9442960\n## new    0.8840023 0.8425232 1.0000000 0.7003630 0.6374712 0.6410886 0.8525682\n## create 0.5720363 0.5415080 0.7003630 1.0000000 0.5907360 0.1469074 0.4126395\n## mech   0.7080738 0.7459097 0.6374712 0.5907360 1.0000000 0.3859502 0.5745533\n## abs    0.6744073 0.4653880 0.6410886 0.1469074 0.3859502 1.0000000 0.5663721\n## math   0.9273116 0.9442960 0.8525682 0.4126395 0.5745533 0.5663721 1.0000000\n# canonical correlation\ncc1 <- cc(sp, ts)"},{"path":"canonical-correlation.html","id":"canonical-correlations","chapter":"7 Canonical Correlation","heading":"7.3.1.2 Canonical correlations","text":"correlations canonical variates (\\(U_1\\), \\(V_1\\)), (\\(U_2\\), \\(V_2\\)), (\\(U_3\\), \\(V_3\\)) respectively.","code":"\n# display the canonical correlations\ncc1$cor## [1] 0.9944827 0.8781065 0.3836057"},{"path":"canonical-correlation.html","id":"raw-canonical-coefficients","chapter":"7 Canonical Correlation","heading":"7.3.1.3 Raw canonical coefficients","text":"raw canonical coefficients interpreted manner analogous interpreting regression coefficients .e., variable growth, one unit increase growth leads 0.062 decrease first canonical variate set 1. variables held constant.","code":"\n# raw canonical coefficients\ncc1[3:4]## $xcoef\n##               [,1]       [,2]       [,3]\n## growth -0.06237788 -0.1740703  0.3771529\n## profit -0.02092564  0.2421641 -0.1035150\n## new    -0.07825817 -0.2382940 -0.3834151\n## \n## $ycoef\n##               [,1]        [,2]        [,3]\n## create -0.06974814 -0.19239132 -0.24655659\n## mech   -0.03073830  0.20157438  0.14189528\n## abs    -0.08956418 -0.49576326  0.28022405\n## math   -0.06282997  0.06831607 -0.01133259"},{"path":"canonical-correlation.html","id":"canonical-loadings","chapter":"7 Canonical Correlation","heading":"7.3.2 Canonical Loadings","text":"Canonical loadings, also known structure coefficients, fundamental concept canonical correlation analysis (CCA). measure simple linear correlation original observed variable (Set X Set Y) corresponding canonical variate respective set. Canonical loadings provide insight variable contributes canonical relationship help identify influential variables set respect canonical variates.Correlations Sales Variables Canonical VariablesAnalyzing first canonical variable related sales, observe uniformly strong negative correlations across variables. Consequently, initial canonical variate can regarded comprehensive gauge Sales Performance. hand, second canonical variable associated Sales Performance displays notably strong correlations, thus offering limited insights data. opted explore third canonical variate pairs.parallel interpretation can applied Test Scores.Correlations test Variables Canonical VariablesBecause correlations large (negative) first canonical variable, can thought overall measure test performance well, however, strongly correlated mathematics test scores. correlations second canonical variable small. suggestion variable may negatively correlated abstract reasoning.\nsee best predictor sales performance mathematics test scores indicator stands .can confirm analysis.Correlations Sales Variables Canonical Variables Test ScoresWe can see three correlations strong show pattern similar canonical variate sales. reason obvious: first canonical correlation high.Correlations Test Scores Canonical Variables Sales VariablesThese also show pattern similar canonical variate test scores. , first canonical correlation high.","code":"\n# compute canonical loadings\ncc2 <- comput(sp, ts, cc1)  \ncc2[3]  ## $corr.X.xscores\n##              [,1]          [,2]         [,3]\n## growth -0.9798776  0.0006477883  0.199598477\n## profit -0.9464085  0.3228847489 -0.007504408\n## new    -0.9518620 -0.1863009724 -0.243414776\n# compute canonical loadings for test scores\ncc2[6]  ## $corr.Y.yscores\n##              [,1]       [,2]        [,3]\n## create -0.6383313 -0.2156981 -0.65140953\n## mech   -0.7211626  0.2375644  0.06773775\n## abs    -0.6472493 -0.5013329  0.57422365\n## math   -0.9440859  0.1975329  0.09422619\n# compute canonical loadings for test scores\ncc2[5]  ## $corr.X.yscores\n##              [,1]          [,2]         [,3]\n## growth -0.9744713  0.0005688272  0.076567107\n## profit -0.9411869  0.2835272081 -0.002878734\n## new    -0.9466102 -0.1635921013 -0.093375287\n# compute canonical loadings for test scores\ncc2[4]  ## $corr.Y.xscores\n##              [,1]       [,2]        [,3]\n## create -0.6348095 -0.1894059 -0.24988439\n## mech   -0.7171837  0.2086069  0.02598458\n## abs    -0.6436782 -0.4402237  0.22027544\n## math   -0.9388771  0.1734549  0.03614570"},{"path":"canonical-correlation.html","id":"do-canonical-correlation-in-our-inbuilt-app","chapter":"7 Canonical Correlation","heading":"7.4 Do Canonical Correlation in our inbuilt app","text":"Good news!\nadded app can simply upload csv get results running cloud server!.","code":""},{"path":"manova.html","id":"manova","chapter":"8 MANOVA","heading":"8 MANOVA","text":"MANOVA, stands Multivariate Analysis Variance, statistical technique used analyzing relationship multiple dependent variables one independent variables. extends principles Analysis Variance (ANOVA) cases two response variables, making suitable situations want investigate joint variation multiple outcome measures response one factors treatments.key points MANOVA advantages ANOVA:Multiple Dependent Variables: MANOVA designed situations one dependent variable. contrast, ANOVA typically used single dependent variable.Simultaneous Analysis: MANOVA allows analyze relationships multiple dependent variables independent variables simultaneously. advantageous considers correlations dependent variables, providing comprehensive view data.Reduced Risk Type Error: analyzing multiple dependent variables together, MANOVA can reduce risk making Type error (rejecting null hypothesis ’s true) compared conducting multiple ANOVAs separately. helps maintain overall significance level analysis.Increased Statistical Power: MANOVA can enhance statistical power detect differences relationships takes account information dependent variables. ANOVA, analyzing variable separately may lead loss information statistical power.Multivariate Test Statistics: MANOVA employs multivariate test statistics (e.g., Wilks’ Lambda, Pillai’s Trace, Hotelling-Lawley Trace, Roy’s Largest Root) assess overall relationship sets variables, allowing nuanced analysis ANOVA’s univariate F-test.summary, MANOVA valuable statistical technique situations involving multiple correlated dependent variables. primary advantages ANOVA include ability analyze variables together, reduce risk Type errors, increase statistical power, provide comprehensive assessment relationships within data. particularly useful studying complex experimental observational data multiple outcome measures.","code":""},{"path":"manova.html","id":"practical-example-3","chapter":"8 MANOVA","heading":"8.1 Practical Example","text":"use real experiment data 3 characters avgR, avgG, avgB 4 groups banana measured. use MANOVA test whether significant difference groups based characters.\ncan see download data :","code":""},{"path":"manova.html","id":"dataset-import-2","chapter":"8 MANOVA","heading":"8.2 Dataset import","text":"First prepare data set similar save csv file. import data set R. See chapter 2to know save csv file import R. Also can directly use code import dataset computer R.Please note path copied system format C:\\Users\\HP\\Documents\\, change format C:/Users/HP/Documents/ R.","code":"\ndata <- read.csv(\"path to your file\")\n#your file will be saved in the name data"},{"path":"manova.html","id":"manova-using-r","chapter":"8 MANOVA","heading":"8.3 MANOVA using R","text":"Interpretation:\nMANOVA results suggest “stage” factor significant multivariate effect response variables. multivariate response variables, taken together, show significant relationship “stage” factor. low p-value (close zero) significance codes (“***“) indicate effect ”stage” factor highly significant. means significant differences multivariate response patterns among different stages.","code":"\n# Performing MANOVA\nattach(data)\nres.man <- manova(cbind(avgR,avgG,avgB) ~ stage, data = data)\nres<-summary(res.man)  \nres##           Df  Pillai approx F num Df den Df    Pr(>F)    \n## stage      3 0.57682   6.0304      9    228 1.387e-07 ***\n## Residuals 76                                             \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"manova.html","id":"saving-result-as-csv","chapter":"8 MANOVA","heading":"8.3.1 Saving result as csv","text":"","code":"\n# Saving results\nres_man<-res$stats\nwrite.csv(res_man, \"result_org.csv\")\n\n# Use this command to see where the result is saved  \ngetwd()"},{"path":"references.html","id":"references","chapter":"9 References","heading":"9 References","text":"","code":""}]

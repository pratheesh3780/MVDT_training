[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"\n\n\nWelcome “Multivariate Data Analysis Tools Agricultural Research” training manual, specialized resource tailored agricultural researchers leveraging R programming language data analysis. realm agricultural research, multidimensional datasets hold keys improving crop yields, soil health, sustainable farming practices, manual equips essential knowledge skills unlock actionable insights. fundamental concepts practical applications, training explores techniques like PCA, Factor Analysis, Cluster Analysis, , within context agricultural research. power R fingertips, ’ll harness full potential agricultural data, making informed decisions drive innovation advancement field agriculture.\n\ncontent manual carefully designed ensure presented simple straightforward manner, making accessible individuals levels expertise. aim demystify complex concepts, providing clarity ease understanding anyone, regardless background, can grasp apply fundamental principles multivariate data analysis agricultural research confidence.\n","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"\n\n\nNote: training Manual published MeLoN (Module e-Learning & Online Notes) Department Agricultural Statistics . online version book free read .\n\nfeedback, please feel free contact Dr.Pratheesh P. Gopinath. E-mail: pratheesh.pg@kau.Thank !\n","code":""},{"path":"r-and-r-studio.html","id":"r-and-r-studio","chapter":"1 R and R studio","heading":"1 R and R studio","text":"training program tailored equip skills needed multivariate data analysis using R RStudio. introductory section, provide brief overview guide process installing R RStudio.","code":""},{"path":"r-and-r-studio.html","id":"r","chapter":"1 R and R studio","heading":"1.1 R","text":"R programming language statistical computing graphics supported R Core Team R Foundation Statistical Computing. Created statisticians Ross Ihaka Robert Gentleman. R implementation S programming language. R used among data miners, bioinformaticians statisticians data analysis developing statistical software. Users created packages augment functions R language.\nAccording user surveys studies scholarly literature databases, R one commonly used programming languages used data mining. official R software environment open-source free software environment within GNU package, available GNU General Public License. written primarily C, Fortran, R (partially self-hosting). R command line interface. Multiple third-party graphical user interfaces also available, RStudio, integrated development environment.\nFigure 1.1: R logo\n","code":""},{"path":"r-and-r-studio.html","id":"rstudio","chapter":"1 R and R studio","heading":"1.2 Rstudio","text":"RStudio integrated development environment (IDE) R. includes console, syntax-highlighting editor supports direct code execution, well tools plotting, history, debugging workspace management. RStudio available open source commercial editions runs desktop (Windows, Mac, Linux) browser connected RStudio Server. RStudio free open-source integrated development environment (IDE) R, programming language statistical computing graphics. JJ Allaire, creator programming language ColdFusion, founded RStudio. RStudio available two editions: RStudio Desktop, program run locally regular desktop application; RStudio Server, allows accessing RStudio using web browser running remote Linux server.RStudio written C++ programming language uses Qt framework graphical user interface. Work RStudio started around December 2010, first public beta version (v0.92) officially announced February 2011.\nFigure 1.2: R studio logo\nTypical RStudio window four panes explained \nFigure 1.3: R studio window\nConsoleThis action happens. authentic R code typed ‘>’ prompt executed pressing ‘Enter’ generate output. going type single call function start app data analysis.Source EditorThis R scripts (collection code) can created edited. New R script can opened clicking File –> New File –> R Script using short cut ctrl+shift+N. can type codes . run code console execute, place cursor line code written press ctrl+enter highlight code wish evaluate clicking “Run” button top right Source. can save R codes, written script.Environment|History|ConnectionsAll data objects (vectors, matrices, dataframes) defined current R session listed Environment tab panel. data objects, may also examine details like quantity observations rows. clickable options available tab, Import Dataset, launch graphical user interface (GUI) inputting data R.panel’s History tab provides history code previously evaluated Console.Environment / History panel helpful become accustomed R. However, can ignore right now. can simply just reduce window clicking minimise button panel’s upper right corner wish clear space screen.Files|Plots|Packages|Help|ViewerYou can find collection useful information Files|Plots|Packages|Help panel. Let’s examine tab detail:\n- file directory hard disc accessible files panel. can utilise “Files” panel set working directory clicking “” “Set Working Directory” ’ve navigated folder wish read save files.plots displayed Plots panel. buttons export plot pdf jpg open plot separate window.\ninterested install button pane, install packages required analysis.","code":""},{"path":"r-and-r-studio.html","id":"installing-r","chapter":"1 R and R studio","heading":"1.3 Installing R","text":"Follow steps correct order installation R","code":""},{"path":"r-and-r-studio.html","id":"first-install-r-latest-version","chapter":"1 R and R studio","heading":"1.3.1 First install R latest version","text":"install R Windows OS:Go CRAN website.Click “Download R Windows”.Click “install R first time”.\nClick “Download R-4.3.1 windows” link download R executable (.exe) file.(time writing manual R version 4.3.1, may change future)downloading file. Run R executable file double clicking downloaded file start installation, allow app make changes device.Select installation language.Follow installation instructions.Click next wait installation complete.Click “Finish” exit complete installation setup.can now run R start menu shortcut created desktop","code":""},{"path":"r-and-r-studio.html","id":"installing-rstudio","chapter":"1 R and R studio","heading":"1.4 Installing Rstudio","text":"installing latest version R. Go Rstudio website.\nFigure 1.4: FALSE\nNow like completed installation R, complete installation setup Rstudio. installation can now run R start menu shortcut created desktop.Now ready explore world data analysis unleash potential R RStudio unlock valuable insights datasets. Happy Statistics R!","code":""},{"path":"r-and-r-studio.html","id":"the-above-descriptions-are-sourced-from","chapter":"1 R and R studio","heading":"The above descriptions are sourced from:","text":"R Project Statistical ComputingRStudioWikipedia R (programming language)Wikipedia RStudio","code":""},{"path":"import.html","id":"import","chapter":"2 Importing data files in R","heading":"2 Importing data files in R","text":"previous chapter discussed basics R\nprogramming including installation, launching, basic data types \narithmetic functions. , learn import data R. \nimportant ensure data well prepared importing\nR avoid errors.","code":""},{"path":"import.html","id":"preparing-your-file","chapter":"2 Importing data files in R","heading":"2.1 Preparing your file","text":"File can prepared MS ExcelFile can prepared MS ExcelUse first row column headers (column names). Generally,\ncolumns represent variables.Use first row column headers (column names). Generally,\ncolumns represent variables.Use first column row names. Generally rows represent\nobservations.Use first column row names. Generally rows represent\nobservations.Make sure row name unique. case \nanalysis experiments , row name treatment name,\nrepeated replicationMake sure row name unique. case \nanalysis experiments , row name treatment name,\nrepeated replicationColumn names compatible R naming conventions.","code":""},{"path":"import.html","id":"naming-conventions","chapter":"2 Importing data files in R","heading":"2.1.1 Naming conventions:","text":"Avoid names blank spaces. Bad column name Sepal width; Good\nconvention Sepal_widthAvoid names blank spaces. Bad column name Sepal width; Good\nconvention Sepal_widthAvoid names special symbols: ?, $, *, +, #, (, ), -, /, }, {,\n|, >, < etc. underscore can used.Avoid names special symbols: ?, $, *, +, #, (, ), -, /, }, {,\n|, >, < etc. underscore can used.Avoid beginning variable names number. Use letter instead.\nGood column names: obs_100m x100m. Bad column name: 100mAvoid beginning variable names number. Use letter instead.\nGood column names: obs_100m x100m. Bad column name: 100mColumn names must unique. Duplicated names allowed.Column names must unique. Duplicated names allowed.R case sensitive. means Name, NAME name, naMe \ntreated different.R case sensitive. means Name, NAME name, naMe \ntreated different.Avoid blank rows dataAvoid blank rows dataDelete comments fileDelete comments fileReplace missing values NA (denotes Available)Replace missing values NA (denotes Available)column containing date, use four digit format.\nGood format: 01/01/2016. Bad format: 01/01/16If column containing date, use four digit format.\nGood format: 01/01/2016. Bad format: 01/01/16A final good looking file\nFigure 2.1: example file\n","code":""},{"path":"import.html","id":"saving-file","chapter":"2 Importing data files in R","heading":"2.1.2 Saving file","text":"recommend save file .csv (comma separated value file)\nformat.CSV?usual file save MS Excel saved XLS files XLSX\nfiles. Workbook files Microsoft Excel 97 2003 known \nXLS files. XLSX extension used later versions Excel. \ndata worksheets workbook, including formatting,\ncharts, graphics, calculations, , contained XLS \nXLSX file formats.Comma Separated Values (CSV) format plain text format \nvalues separated commas, whereas Excel Sheets binary file\nformat (XLS) contains information worksheets file,\nincluding content formatting. spreadsheet programme,\nincluding Microsoft Excel, Open Office, Google Sheets, etc., can open\nCSV files. straightforward text editor can also used open CSV\nfiles. straightforward compatible majority \nplatforms, prevalent well-liked file format storing\naccessing data. certain drawbacks simplicity.\nCSV files can contain single sheet without formatting \nformulas.CSV files supported almost data upload interfaces,\nExcel (XLS XLSX) file types preferable storing \ncomplicated data. CSV file format may advantageous \nintend move data platforms export import \ninterfaces.","code":""},{"path":"import.html","id":"how-to-save-as-csv","chapter":"2 Importing data files in R","heading":"2.1.3 How to save as csv","text":"\"File name\" section \"Save \" tab, can select\n\"Save type\" change \"CSV (Comma delimited) (*.csv).\nFigure 2.2: save csv\n\nFigure 2.3: save csv (1)\n","code":""},{"path":"import.html","id":"importing-data-set-in-rstudio","chapter":"2 Importing data files in R","heading":"2.2 Importing Data set in Rstudio","text":"import csv file Rstudioclick File click Import Dataset select Text (base)\nFigure 2.4: Importing data set\nSelect file click open\nFigure 2.5: Import Dataset dialogue box\nImport Dataset dialogue box can change name \ndataset Box Name. Heading radio button default\n‘yes’. Click import. dataset now imported ready \nwork .","code":""},{"path":"import.html","id":"alternate-methods","chapter":"2 Importing data files in R","heading":"2.2.1 Alternate methods","text":"","code":""},{"path":"import.html","id":"importing-csv-files","chapter":"2 Importing data files in R","heading":"2.2.1.1 Importing csv files","text":"Data can also imported using read.csv() function R.\nread.csv('path file')","code":"\n# example\n\nmy_data<-read.csv(file = 'csv/usarrests.csv')\n\n# here now the data set usarrests.csv stored in folder csv is stored in the name my_data\n\n# You can now directly do operations on the my_data\n\nsummary(my_data)##       X                 Murder          Assault         UrbanPop    \n##  Length:50          Min.   : 0.800   Min.   : 45.0   Min.   :32.00  \n##  Class :character   1st Qu.: 4.075   1st Qu.:109.0   1st Qu.:54.50  \n##  Mode  :character   Median : 7.250   Median :159.0   Median :66.00  \n##                     Mean   : 7.788   Mean   :170.8   Mean   :65.54  \n##                     3rd Qu.:11.250   3rd Qu.:249.0   3rd Qu.:77.75  \n##                     Max.   :17.400   Max.   :337.0   Max.   :91.00  \n##       Rape      \n##  Min.   : 7.30  \n##  1st Qu.:15.07  \n##  Median :20.10  \n##  Mean   :21.23  \n##  3rd Qu.:26.18  \n##  Max.   :46.00"},{"path":"import.html","id":"importing-excel-files","chapter":"2 Importing data files in R","heading":"2.2.1.2 Importing excel files","text":"import xlsx file, need package xlsx","code":"\nlibrary(xlsx)  \n\ndf <-read.xlsx(\"path/file.xlsx\", n)\n\n# n is n-th worksheet to import"},{"path":"principal-component-analysis-pca.html","id":"principal-component-analysis-pca","chapter":"3 Principal Component Analysis (PCA)","heading":"3 Principal Component Analysis (PCA)","text":"","code":""},{"path":"principal-component-analysis-pca.html","id":"what-is-pca","chapter":"3 Principal Component Analysis (PCA)","heading":"3.1 What is PCA?","text":"Principal component analysis (PCA) technique transforms high-dimensions data lower-dimensions retaining much information possible. PCA invented 1909 Karl Pearson, analogue principal axis theorem mechanics; later independently developed named Harold Hotelling 1930s.(Johnson Wichern 2014)Drawing meaningful inferences high-dimensional data can challenging, humans naturally excel visualizing comprehending information two dimensions. PCA, powerful technique, aids transforming multi-dimensional data manageable form reducing dimensionality. simplification facilitates easier visualization analysis, ultimately enhancing ability extract valuable insights complex datasets.PCA valuable tool social science agricultural research. works transforming multi-dimensional data lower-dimensional space retaining much variance data possible. essence, PCA identifies significant dimensions “principal components” data, effectively reducing complexity.\nidea PCA simple — reduce number variables data set, preserving much information possible.","code":""},{"path":"principal-component-analysis-pca.html","id":"when-to-use-pca","chapter":"3 Principal Component Analysis (PCA)","heading":"3.2 When to Use PCA","text":"Dimensionality Reduction: Use PCA high-dimensional dataset many features (variables) want reduce dimensionality. can help cases many variables work efficiently.Data Visualization: PCA effective need visualize high-dimensional data. projecting data onto lower-dimensional space, can create scatter plots, heatmaps, visualizations easier interpret.Minimum data set: Principal components can used eliminate data sets identify minimum data set experimentation.Noise Reduction: dataset contains noisy redundant features, PCA can help capturing important information eliminating less relevant components.Multicollinearity: dataset multicollinearity issues (high correlations variables), PCA can help reduce interdependencies, making models stable interpretable.Sample size (n) least equal number dimensions (n ≥ p)","code":""},{"path":"principal-component-analysis-pca.html","id":"when-not-to-use-pca","chapter":"3 Principal Component Analysis (PCA)","heading":"3.3 When Not to Use PCA","text":"sample size less number dimensions (n < p)Non-Linear Relationships: PCA based linear transformations may effective data contains complex non-linear relationships. cases, techniques like kernel PCA non-linear dimensionality reduction methods might appropriate.Small Dimensionality: already low-dimensional dataset important variables, applying PCA might provide significant benefits even lead information loss.Loss Variability Information: PCA aims maximize variance capture, may desirable cases. preserving characteristics data important (e.g., categorical information), dimensionality reduction techniques considered.","code":""},{"path":"principal-component-analysis-pca.html","id":"how-pca-is-done","chapter":"3 Principal Component Analysis (PCA)","heading":"3.4 How PCA is done","text":"training program, primarily focused practical application PCA rather delving theoretical aspects. aim explore PCA can effectively utilized understand interpret results meaningful manner. usual procedure performing follows information:-\n* Standardize range continuous initial variables\n* Compute covariance matrix identify correlations\n* Compute eigen vectors eigen values covariance matrix identify principal components\n* Create feature vector decide principal components keep\n* Recast data along principal components axes","code":""},{"path":"principal-component-analysis-pca.html","id":"what-is-principal-component","chapter":"3 Principal Component Analysis (PCA)","heading":"3.5 What is Principal Component","text":"Principal components new variables constructed linear combinations initial variables. combinations done way new variables (.e. principal components) uncorrelated information within initial variables included first components. , idea 10-dimensional data gives 10 principal components, PCA tries put maximum possible information first component, maximum remaining information second .example scree plot shown can see 5 principal components 5-dimensional data corresponding variance explained.\nFigure 3.1: Scree Plot: Principal components percentage variance explained\n","code":""},{"path":"principal-component-analysis-pca.html","id":"how-pca-constructs-the-principal-components","chapter":"3 Principal Component Analysis (PCA)","heading":"3.6 How PCA Constructs the Principal Components","text":"Number principal components equal number variables data, principal components constructed manner first principal component accounts largest possible variance data set. example, see Figure 1.2 , can see scatter plot assumed data set, can guess first principal component ? Yes, ’s approximately line matches purple marks goes origin ’s line projection points (red dots) spread . mathematically speaking, ’s line maximizes variance (average squared distances projected points (red dots) origin).\nFigure 3.2: Concept PCA\nlines (PCs) identified using linear algebra concepts Eigen vectors eigen values calculated covariance matrix order determine principal components data. going much theoretical details.","code":""},{"path":"principal-component-analysis-pca.html","id":"practical-example","chapter":"3 Principal Component Analysis (PCA)","heading":"3.7 PRACTICAL EXAMPLE","text":"using Usarrests dataset R explain PCA coming sessions.“USarrests” dataset R built-dataset offers insights crime arrests across 50 states United States 1973. comprises four key variables: murder rate, assault rate, percentage population living urban areas, rape rate.can View download datasets ","code":""},{"path":"principal-component-analysis-pca.html","id":"analysis","chapter":"3 Principal Component Analysis (PCA)","heading":"3.8 Analysis","text":"First prepare data set save csv file. import data set R. See chapter 2 know save csv file import R. can directly use code import dataset computer R.Please note path copied system format C:\\Users\\HP\\Documents\\, change format C:/Users/HP/Documents/ R.\n(#fig:data_imp)Data set importing R Rstudio\nusing package factoextra performing analysisInstall required packages:Follow codes PCA analysis","code":"\ndata <- read.csv(\"path to your file\", row.names=1)\ninstall.packages(\"factoextra\")  \nlibrary(factoextra)\n#storing your dataset to the variable data\ndata <- read.csv(\"csv/usarrests.csv\", row.names=1)\n\n#this will display the first 6 rows of your data\nhead(data, n = 6)##            Murder Assault UrbanPop Rape\n## Alabama      13.2     236       58 21.2\n## Alaska       10.0     263       48 44.5\n## Arizona       8.1     294       80 31.0\n## Arkansas      8.8     190       50 19.5\n## California    9.0     276       91 40.6\n## Colorado      7.9     204       78 38.7\n# Do PCA using prcomp function in factoextra\npca_res <- prcomp(data, scale = TRUE)\n# scale = TRUE will standardise the variables (x-mean(x)/sd(x))\nsummary(pca_res)## Importance of components:\n##                           PC1    PC2     PC3     PC4\n## Standard deviation     1.5749 0.9949 0.59713 0.41645\n## Proportion of Variance 0.6201 0.2474 0.08914 0.04336\n## Cumulative Proportion  0.6201 0.8675 0.95664 1.00000"},{"path":"principal-component-analysis-pca.html","id":"deciding-on-the-number-of-pcs","chapter":"3 Principal Component Analysis (PCA)","heading":"3.9 Deciding on the number of PCs","text":"","code":""},{"path":"principal-component-analysis-pca.html","id":"scree-plot","chapter":"3 Principal Component Analysis (PCA)","heading":"3.9.1 Scree Plot","text":"scree plot helps decide many principal components retain PCA analysis. choice number components can vary depending specific goals, ’s often based combination statistical criteria, explained variance elbow point, well domain knowledge interpretability.key inferences can make plot:Explained Variance: scree plot displays proportion total variance explained principal component. Inferences can made examining much variance explained component. components left contribute variance, right contribute less.Elbow Point: Look “elbow” point explained variance sharply decreases. often good indicator number principal components retain. point just explained variance starts level can suitable choice. ’s point adding components doesn’t explain much additional variance.Cumulative Variance: can also examine cumulative explained variance. scree plot may show cumulative curve increases components added. can look point cumulative variance reaches satisfactory level (e.g., 70%, 80%, 90%) determine number components retain.Interpretability: Consider interpretability components. Sometimes, might choose retain components even explain less variance meaningful interpretations context.Domain Knowledge: Always consider domain subject matter knowledge deciding number components retain. Sometimes, context analysis may dictate number components practically meaningful.","code":"\n# Visualize eigenvalues (scree plot)\nplot1 <- fviz_eig(pca_res,addlabels = TRUE)\nplot1\n# Drawing an elbow point if needed\npca.var =pca_res$sdev ^2\nvar.ratio=pca.var/sum(pca.var)\nplot(var.ratio , xlab=\" Principal Component \", ylab=\" Proportion of\nVariance Explained \", ylim=c(0,1) ,type=\"b\")"},{"path":"principal-component-analysis-pca.html","id":"eigen-values","chapter":"3 Principal Component Analysis (PCA)","heading":"3.9.2 Eigen Values","text":"“Eigenvalues” represent variance explained principal component (PC) PCA. Percentage variance ratio eigen value principal component sum eigen values PCs. important note first two PCs atleast explain 80% variance data.Eigenvalues can used determine number principal components retain PCA (Kaiser 1961):eigenvalue > 1 indicates PCs account variance accounted one original variables standardized data. commonly used cutoff point PCs retained. holds true data standardized.can also select number principal components Principal Component Analysis (PCA) based desired level explained variance. instance, want retain 80% total variance explained, can choose number components achieves level.","code":"\n# Eigenvalues\neig.val <- get_eigenvalue(pca_res)\neig.val##       eigenvalue variance.percent cumulative.variance.percent\n## Dim.1  2.4802416        62.006039                    62.00604\n## Dim.2  0.9897652        24.744129                    86.75017\n## Dim.3  0.3565632         8.914080                    95.66425\n## Dim.4  0.1734301         4.335752                   100.00000\n#Now we will take the PCA results keeping first two pricipal components only\nres.pca <- prcomp(USArrests, scale = TRUE, rank =2)\n#here rank =2 will keep only two PCs"},{"path":"principal-component-analysis-pca.html","id":"accessing-the-pca-results","chapter":"3 Principal Component Analysis (PCA)","heading":"3.10 Accessing the PCA results","text":"Please note results discussed Dim1 , Dim2 etc denotes Principal Component1(PC1), PC2 etc respectively. now discuss terms PCA can interpreted get meaningful insights.","code":""},{"path":"principal-component-analysis-pca.html","id":"pc-loadings","chapter":"3 Principal Component Analysis (PCA)","heading":"3.10.1 PC Loadings","text":"Loadings coefficients linear combination predicting variable (standardized) components. Loadings represent weights assigned original variable linear combination forms principal component. weights indicate importance variable creating component. statistical language loadings eigenvectors scaled square roots respective eigenvalues.Positive loadings indicate positive relationship variable component, suggesting increase variable associated increase component’s value. Negative loadings indicate negative relationship, meaning increase variable corresponds decrease component’s value. magnitude loading reflects strength relationship. Larger loadings indicate variable substantial impact component. Loadings typically standardized mean 0 standard deviation 1, ensuring variables different scales directly comparable.first loading vector places approximately equal weight Assault, Murder, Rape, much less weight UrbanPop. Hence component roughly corresponds measure overall rates serious crimes. loadings negative can assume states scoring lesser values PC1 higher crime rate.","code":"\n#getting PC loadings\nres.pca$rotation##                 PC1        PC2\n## Murder   -0.5358995 -0.4181809\n## Assault  -0.5831836 -0.1879856\n## UrbanPop -0.2781909  0.8728062\n## Rape     -0.5434321  0.1673186"},{"path":"principal-component-analysis-pca.html","id":"variable-coordinates","chapter":"3 Principal Component Analysis (PCA)","heading":"3.10.2 Variable coordinates","text":"coordinates provides insights relative positions variables PCA space relate principal components geometrically. Absolute value measures gives strength association variable particular PCs. can avoid final tables, presenting results. coordinates already represented biplots.","code":"\n# Results for Variables\nres.var <- get_pca_var(res.pca)\nres.var$coord          # Coordinates  ##               Dim.1      Dim.2      Dim.3       Dim.4\n## Murder   -0.8439764 -0.4160354 -0.3200012 -0.17415116\n## Assault  -0.9184432 -0.1870211 -0.3482359 -0.07828649\n## UrbanPop -0.4381168  0.8683282 -0.1661159  0.36347960\n## Rape     -0.8558394  0.1664602 -0.3244991  0.06967974"},{"path":"principal-component-analysis-pca.html","id":"variable-contributions","chapter":"3 Principal Component Analysis (PCA)","heading":"3.10.3 Variable contributions","text":"measures provides percentage contributions variable principal component. indicates proportion variance explained variable principal component. Higher values suggest variable substantial influence formation respective component. helps identify variables contribute significantly variance explained component useful variable selection interpretation.contributions reflect idea loadings convey. can see higher contribution Assault, Murder, Rape PC1 higher contribution urban population PC2.","code":"\nres.var$contrib        # Contributions to the PCs  ##              Dim.1     Dim.2     Dim.3     Dim.4\n## Murder   28.718825 17.487524 28.718825 17.487524\n## Assault  34.010315  3.533859 34.010315  3.533859\n## UrbanPop  7.739016 76.179065  7.739016 76.179065\n## Rape     29.531844  2.799553 29.531844  2.799553"},{"path":"principal-component-analysis-pca.html","id":"cos2-representation","chapter":"3 Principal Component Analysis (PCA)","heading":"3.11 cos2 representation","text":"Cosine Squared (Cos²) Values: cos2 values provides cosine squared values variable respect principal component. Cosine squared values measure much variable’s variance explained corresponding principal component. cos2 values typically range 0 1. value 1 indicates variable perfectly aligned principal component, meaning variable’s entire variance explained component. value 0 indicates variable orthogonal component.\nHigher cos2 values indicate variable well-represented principal component. variables contribute significantly explanation variance along component. Lower values suggest variable less alignment component less influential explaining variance.can exclude values final table presenting results.","code":"\nres.var$cos2           # Quality of representation ##              Dim.1     Dim.2      Dim.3       Dim.4\n## Murder   0.7122962 0.1730854 0.10240075 0.030328628\n## Assault  0.8435380 0.0349769 0.12126826 0.006128774\n## UrbanPop 0.1919463 0.7539938 0.02759448 0.132117419\n## Rape     0.7324611 0.0277090 0.10529968 0.004855266"},{"path":"principal-component-analysis-pca.html","id":"measures-of-indviduals","chapter":"3 Principal Component Analysis (PCA)","heading":"3.11.1 Measures of Indviduals","text":"Similar measures calculated individuals also. individuals higher values PCs can identified insights can drawn based .\nexample states high negative values PC1 high crime rate states high values PC2 higher level urbanization.\nprincipal components scores vector 50 states(indviduals) can viewed using following code.California, Nevada Florida, high crime rates large negative scores first component, states like North Dakota, positive scores first component, low crime rates.California also high score second component, indicating high level urbanization, opposite true states like Mississippi.can find contributions, cos2 coordinates individuals, depth analysis required study, using code .","code":"\n# principal components scores vector for all 50 states(indviduals)\nres.pca$x##                        PC1         PC2\n## Alabama        -0.97566045 -1.12200121\n## Alaska         -1.93053788 -1.06242692\n## Arizona        -1.74544285  0.73845954\n## Arkansas        0.13999894 -1.10854226\n## California     -2.49861285  1.52742672\n## Colorado       -1.49934074  0.97762966\n## Connecticut     1.34499236  1.07798362\n## Delaware       -0.04722981  0.32208890\n## Florida        -2.98275967 -0.03883425\n## Georgia        -1.62280742 -1.26608838\n## Hawaii          0.90348448  1.55467609\n## Idaho           1.62331903 -0.20885253\n## Illinois       -1.36505197  0.67498834\n## Indiana         0.50038122  0.15003926\n## Iowa            2.23099579  0.10300828\n## Kansas          0.78887206  0.26744941\n## Kentucky        0.74331256 -0.94880748\n## Louisiana      -1.54909076 -0.86230011\n## Maine           2.37274014 -0.37260865\n## Maryland       -1.74564663 -0.42335704\n## Massachusetts   0.48128007  1.45967706\n## Michigan       -2.08725025  0.15383500\n## Minnesota       1.67566951  0.62590670\n## Mississippi    -0.98647919 -2.36973712\n## Missouri       -0.68978426  0.26070794\n## Montana         1.17353751 -0.53147851\n## Nebraska        1.25291625  0.19200440\n## Nevada         -2.84550542  0.76780502\n## New Hampshire   2.35995585  0.01790055\n## New Jersey     -0.17974128  1.43493745\n## New Mexico     -1.96012351 -0.14141308\n## New York       -1.66566662  0.81491072\n## North Carolina -1.11208808 -2.20561081\n## North Dakota    2.96215223 -0.59309738\n## Ohio            0.22369436  0.73477837\n## Oklahoma        0.30864928  0.28496113\n## Oregon         -0.05852787  0.53596999\n## Pennsylvania    0.87948680  0.56536050\n## Rhode Island    0.85509072  1.47698328\n## South Carolina -1.30744986 -1.91397297\n## South Dakota    1.96779669 -0.81506822\n## Tennessee      -0.98969377 -0.85160534\n## Texas          -1.34151838  0.40833518\n## Utah            0.54503180  1.45671524\n## Vermont         2.77325613 -1.38819435\n## Virginia        0.09536670 -0.19772785\n## Washington      0.21472339  0.96037394\n## West Virginia   2.08739306 -1.41052627\n## Wisconsin       2.05881199  0.60512507\n## Wyoming         0.62310061 -0.31778662\nres.ind <- get_pca_ind(res.pca)\nres.ind$coord          # Coordinates\nres.ind$contrib        # Contributions to the PCs\nres.ind$cos2           # Quality of representation "},{"path":"principal-component-analysis-pca.html","id":"biplot-indviduals","chapter":"3 Principal Component Analysis (PCA)","heading":"3.12 Biplot (Indviduals)","text":"“Biplot Individuals” graphical representation used Principal Component Analysis (PCA) multivariate statistical methods visualize individual data points observations related underlying patterns variables dataset. examining biplot, analysts researchers can gain insights various aspects data, including clustering patterns among individual observations.Interpretation Biplot indvidualsIndividuals’ Position: point biplot represents individual observation dataset. positions points plot show individual relates principal components. Individuals closer plot similar terms relationships principal components.Color Coding: points representing individuals may color-coded based “cos2” values, indicate quality representation individual plot. Higher “cos2” values indicate individual’s position plot strongly associated principal components.“cos2” Biplot:\ncontext PCA biplot, “cos2” represents square cosine angle individual’s position (point) variable’s arrow plot. quantifies quality representation individual plot.High “cos2” values suggest individual’s position plot well-represented variables principal components.High “cos2” values suggest individual’s position plot well-represented variables principal components.Low “cos2” values suggest individual’s position may well-represented may noisy plotLow “cos2” values suggest individual’s position may well-represented may noisy plot","code":"\n#Graph of individuals. Individuals with a similar profile are grouped together (Biplot)\nbiplot1<-fviz_pca_ind(res.pca,\n             col.ind = \"cos2\", # Color by the quality of representation\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE     # Avoid text overlapping\n             )\nbiplot1"},{"path":"principal-component-analysis-pca.html","id":"biplot-variables","chapter":"3 Principal Component Analysis (PCA)","heading":"3.13 Biplot (variables)","text":"biplot, variables represented arrows vectors, plot allows examination variables associated principal components.Interpretation PCA Variable BiPlot:Variable Positions: point variable plot represents variable dataset. positions points plot show variable relates principal components. Variables closer plot similar terms relationships principal components.Direction Arrows: Arrows variable plot represent variables contribution principal components. direction arrows indicates variable associated principal component. Variables pointing direction positively correlated corresponding component, pointing opposite directions negatively correlated. helps understand variables move direction component move opposite direction.Arrow Length: length arrows indicates strength relationship variables principal components. Longer arrows represent variables higher contribution component.Color Coding: points representing variables may color-coded based contributions principal components. Higher contributions typically associated darker color.","code":"\n#Graph of variables. \nbiplot2<-fviz_pca_var(res.pca,\n             col.var = \"contrib\", # Color by contributions to the PC\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE     # Avoid text overlapping\n             )\nbiplot2"},{"path":"principal-component-analysis-pca.html","id":"biplot-indviduals-and-variables","chapter":"3 Principal Component Analysis (PCA)","heading":"3.14 Biplot (Indviduals and variables)","text":"biplot combines individual data points represented points plot variables represented arrows vectors. provides powerful way assess relationships individual observations variables, offering insights patterns, clusters, associations within data. examining positions individuals directions variable vectors, analysts can gain holistic understanding data’s underlying structure, making valuable tool dimensionality reduction data exploration.Interpretation PCA BiPlot (Individuals Variables):Distance Origin (Center): distance individual point origin (center) plot represents contribution explained variance principal components. Points origin stronger influence principal components, points closer origin weaker influence. words, individuals origin better represented principal components.Projection onto Arrows: position individual point relative arrows tells individual influenced variables. individual point projected close tip arrow, indicates individual’s profile strongly influenced particular variable represented arrow.Alignment Points Arrows: direction arrows position individuals align, suggests variables individuals positively correlated corresponding principal component. words, variables point direction individual points contribute positively principal component.Opposite Direction: individual point arrow opposite directions, indicates negative correlation individual variable, implying variable contributes negatively principal component individual.summary, relationship position individuals direction arrows PCA biplot provides insights individual observations relate variables principal components. biplot helps understand strength direction relationships, allowing identify variables influence components individuals affected variables.biplot shows 50 states mapped according 2 principal components. vectors PCA 4 variables also plotted.states California, Nevada Florida,seen towards extreme left, indicating high negative value PC1. .e. states high crime rates.states like North Dakota seen towards right high positive scores first component, indicative low crime rates.general can divide quadrants biplot example follows:States Quadrant towards right: Low crime rate high urbanizationStates Quadrant II towards left: High crime rate high urbanizationStates Quadrant III towards bottom left: High crime rate low urbanizationStates Quadrant IV towards bottom right: Low crime rate low urbanizationTowards center: States moderate crime rate urbanization\nFigure 3.3: Quadrants biplot\nUsing code can save plots required resolution. changing resolution change value res code .","code":"\n#Biplot of individuals and variables\nbiplot3<- fviz_pca_biplot(res.pca, repel = TRUE,\n                col.var = \"#2E9FDF\", # Variables color\n                col.ind = \"#696969\"  # Individuals color\n                )\nbiplot3\n# Save the scree plot as a PNG file\n  png(\"eigenvalues_plot.png\", width = 800, height = 600, units = \"px\", res = 100)\n  plot1\n  dev.off() # Close the PNG device  \n\n  # Save the biplot of indviduals a PNG file\n  png(\"bi_plot.png\", width = 800, height = 600, units = \"px\", res = 100)\n  biplot1\n  dev.off() # Close the PNG device  \n  \n  # Save the biplot of variables a PNG file\n  png(\"bi_plot2.png\", width = 800, height = 600, units = \"px\", res = 100)\n  biplot2\n  dev.off() # Close the PNG device  \n  \n  # Save the biplot of indviduals and variables a PNG file\n  png(\"bi_plot3.png\", width = 800, height = 600, units = \"px\", res = 100)\n  biplot3\n  dev.off() # Close the PNG device  "},{"path":"principal-component-analysis-pca.html","id":"do-pca-in-our-inbuilt-app","chapter":"3 Principal Component Analysis (PCA)","heading":"3.15 Do PCA in our inbuilt app","text":"Good news!\nadded app can simply upload csv get results running cloud server!.","code":""},{"path":"exploratory-factor-analysis.html","id":"exploratory-factor-analysis","chapter":"4 Exploratory Factor Analysis","heading":"4 Exploratory Factor Analysis","text":"Exploratory Factor Analysis (EFA) statistical technique used field multivariate data analysis identify underlying patterns latent variables (hidden variables) explain correlations covariations among set observed variables. dimensionality reduction method commonly employed fields like psychology, sociology, social sciences, well agricultural extension research areas data reduction simplification needed (Johnson Wichern 2014).Latent variable: unobservable hidden variable directly measured inferred estimated based impact observed measurable variables. Latent variables often used statistical modeling explain patterns, relationships, variability data. represent underlying constructs, concepts, factors influence observed data directly observed measured.primary goal EFA uncover latent factors constructs responsible observed patterns data. latent factors unobservable, help explain common variance among observed variables. EFA assume pre-defined relationships variables used data exploration hypothesis generation.Imagine data 50 students, ’ve recorded marks subjects: English, Hindi, French, Physics, Maths, Statistics. want understand underlying factors influence subject scores. Factor analysis can help us understand underlying factors may contribute students’ academic performance. case, may identifies factors related language humanities skills (e.g., English, Hindi, French) analytical skills (e.g., Physics, Maths, Statistics).two types factor analysis, called exploratory confirmatory factor analysis (EFA CFA). EFA CFA aim reproduce observed relationships among group features smaller set latent variables. EFA used descriptive, data-driven manner uncover measured variables reasonable indicators various latent dimensions. contrast, CFA conducted -priori, hypothesis-testing manner requires strong empirical theoretical foundations. discuss EFA , used group features specified number latent factors.","code":""},{"path":"exploratory-factor-analysis.html","id":"practical-example-1","chapter":"4 Exploratory Factor Analysis","heading":"4.1 PRACTICAL EXAMPLE","text":"practical example section, work dataset containing marks 100 students various subjects, including English, Hindi, Maths, French, Physics, Poetry, Statistics, Geometry.can View download datasets ","code":""},{"path":"exploratory-factor-analysis.html","id":"analysis-1","chapter":"4 Exploratory Factor Analysis","heading":"4.2 Analysis","text":"First prepare data set save csv file. import data set R. See chapter 2to know save csv file import R. can directly use code import dataset computer R.Please note path copied system format C:\\Users\\HP\\Documents\\, change format C:/Users/HP/Documents/ R.using pysch package factor analysis.Install required packages:","code":"\ndata <- read.csv(\"path to your file\", row.names=1)\n#your file will be saved in the name data\ninstall.packages(\"psych\")  "},{"path":"exploratory-factor-analysis.html","id":"determining-the-suitability-of-your-dataset-for-efa","chapter":"4 Exploratory Factor Analysis","heading":"4.2.1 Determining the suitability of your dataset for EFA","text":"perform Kaiser–Meyer–Olkin (KMO) test (Kaiser 1970) & (Kaiser Rice 1974) test suitability dataset factor analysis.\nKaiser–Meyer–Olkin (KMO) test statistical measure determine suited data factor analysis. test measures sampling adequacy variable model complete model. Measure Sampling Adequacy (MSA) measure proportion variance among variables might common variance. higher proportion, higher KMO-value, suited data factor analysis.KMO values 0.8 1 indicate adequate sampling data suitable factor analysisKMO values 0.6 suggest inadequate sampling may require remedial action.KMO value close zero indicates presence large partial correlations relative sum correlations, can pose challenge factor analysis.\nperform KMO analysis using library psych R. Use code .example can see Kaiser-Meyer-Olkin (KMO) measure sampling adequacy (MSA), calculated dataset, indicates overall adequacy 0.84, suggesting data suitable factor analysis. Additionally, individual item-specific KMO values subject, including English, Hindi, Maths, French, Physics, Poetry, Statistics, Geometry, range 0.81 0.86, supporting adequacy data factor analysis.\ncase item-specific MSA < 0.60, better avoid item analysis.","code":"\nlibrary(psych)\ndata <- read.csv(\"csv/score.csv\", row.names=1)\nKMO(data)## Kaiser-Meyer-Olkin factor adequacy\n## Call: KMO(r = data)\n## Overall MSA =  0.84\n## MSA for each item = \n##    English      Hindi      Maths     French    Physics     Poetry Statistics \n##       0.84       0.84       0.84       0.84       0.85       0.81       0.86 \n##   Geometry \n##       0.85"},{"path":"exploratory-factor-analysis.html","id":"identifying-the-number-of-factors","chapter":"4 Exploratory Factor Analysis","heading":"4.2.2 Identifying the number of factors","text":"discuss two methods identify number factors. methods can employed.","code":""},{"path":"exploratory-factor-analysis.html","id":"scree-plot-1","chapter":"4 Exploratory Factor Analysis","heading":"4.2.2.1 Scree plot","text":"Scree plot graphical representation used factor analysis identify optimal number factors retain dataset. displays eigenvalues correlation matrix, represent amount variance explained factor. employ Scree plot determine number factors retained given dataset. examining pattern eigenvalues Scree plot, can identify point eigenvalues significantly drop. drop-serves indicator meaningful influential factors, ensuring capture essential dimensions within data.","code":"\nev <- eigen(cor(score)) # get eigenvalues\nev$values## [1] 3.4434243 3.2878668 0.3070565 0.2643156 0.2072886 0.1817183 0.1653205\n## [8] 0.1430094\nscree(score, pc=FALSE)"},{"path":"exploratory-factor-analysis.html","id":"parallel-analysis","chapter":"4 Exploratory Factor Analysis","heading":"4.2.2.2 Parallel analysis","text":"Parallel analysis robust statistical technique employed factor analysis address crucial question many factors retained given dataset. method particularly valuable dealing complex data structures. Rather relying solely conventional rules subjective judgment, parallel analysis offers data-driven approach. involves generating random datasets similar properties original data comparing factor structures. scrutinizing eigenvalues derived random datasets alongside actual data, parallel analysis provides objective reliable means determining optimal number factors retain. approach enhances accuracy factor analysis, ensuring identified factors capture meaningful interpretable dimensions within dataset.Identify point eigenvalues actual data surpass corresponding eigenvalues random data significant margin. indicates number factors retained. example can see number identified factors two.","code":"\nlibrary(psych)\nparallel<-fa.parallel(score, fa=\"fa\")## Parallel analysis suggests that the number of factors =  2  and the number of components =  NA"},{"path":"exploratory-factor-analysis.html","id":"factor-analysis","chapter":"4 Exploratory Factor Analysis","heading":"4.2.3 Factor Analysis","text":"using built-function R factanal() perform factor analysis.","code":"\nNfacs <- 2  # Number of factors as identified in parallel analysis/Scree plot.\nfit <- factanal(score, Nfacs,rotation=\"varimax\",scores=\"regression\")"},{"path":"exploratory-factor-analysis.html","id":"factor-rotation","chapter":"4 Exploratory Factor Analysis","heading":"4.2.3.1 Factor rotation","text":"Rotation context factor analysis mathematical technique used simplify interpret results analysis. Rotation methods applied factor loadings, coefficients indicate strongly observed variable associated latent factor. loadings can challenging interpret factor analysis yields complex, overlapping, unclear results.\nRotation serves following primary purposes:• Simplification\n• Enhanced InterpretabilityTwo type rotation , Orthogonal Oblique. orthogonal rotation, factors forced uncorrelated , simplifying factor structure. oblique rotation, factors can correlated, allowing realistic interpretations factors expected related.R offers two rotation methods, include Varimax (orthogonal), Promax (oblique. Researchers choose appropriate rotation method based hypotheses data goals simplifying interpreting results factor analysis.R rotation option factanal() function may changed “promax” “varimax”. example used Varimax rotation, Promax Varimax rotation case giving results factor analysis, suggests underlying factor structure data may sufficiently simple well-defined. factor solution clear distinct, ’s possible different rotation methods converge solution.","code":""},{"path":"exploratory-factor-analysis.html","id":"factor-loadings","chapter":"4 Exploratory Factor Analysis","heading":"4.2.3.2 Factor loadings","text":"Factor loadings coefficients represent relationships observed variables (indicators) latent factors factor analysis. coefficients indicate strength direction associations variable factor. Factor loadings play crucial role understanding underlying structure data extracting meaningful information factor analysis.’s interpret factor loadings:Magnitude Loadings: magnitude loading represents strength relationship variable factor. Larger loading values indicate stronger association. example, loading 0.7 stronger loading 0.3.Sign Loadings: sign loading (positive negative) indicates direction relationship. positive loading suggests increase variable associated increase factor, negative loading suggests inverse relationship.Interpretation: Interpret factor loadings considering variables high (absolute) loadings factor. Variables high loadings particular factor strongly associated factor contribute significantly .Cross-Loadings: variables may moderate loadings multiple factors. called cross-loadings suggest variable related one factor. Cross-loadings can provide insights complexity data.Loadings Close 0: Loadings close 0 indicate weak negligible relationship variable factor. Variables loadings near 0 contribute significantly factor can often removed analysis simplify factor structure.Naming Factors: Assign meaningful labels factors based variables high loadings. variables load strongly factor can help identify underlying theme concept factor represents.Lets find loadings exampleIn output, factor loadings factor analysis two factors, referred “Factor1” “Factor2.” Factor1 Factor 2 together explains 78.9% variance, can see Cumulative Var result. Proportion Var result gives proportion variance explained factor. loadings represent relationships observed variables latent factors. ’s interpretation loadings:“Factor1”:\n- variables “Maths,” “Physics,” “Statistics,” “Geometry” high positive loadings, values around 0.88 0.91. indicates strongly associated “Factor1.”“Factor1” seems represent set variables related quantitative analytical subjects, given strong loadings mathematics-related subjects “Maths,” “Physics,” “Statistics.” Factor 1 can named analytical ability.“Factor2”:\n- variables “English,” “Hindi,” “French,” “Poetry” high positive loadings, around 0.85 0.91, “Factor2.” loadings suggest strong association “Factor2.”“Factor2” appears capture different set variables, likely related language literature, given strong loadings languages “Poetry.”interpretation suggests factors distinct represent different aspects data. “Factor1” associated quantitative subjects, “Factor2” associated language literature-related variables. Factor 2 can named “Linguistic aptitude”.can see loadings blank. fact, loadings smaller 0.1 absolute value omitted (sparse structure displayed clearly).","code":"\nfit$loadings## \n## Loadings:\n##            Factor1 Factor2\n## English             0.877 \n## Hindi               0.879 \n## Maths       0.909         \n## French              0.849 \n## Physics     0.901         \n## Poetry              0.910 \n## Statistics  0.894         \n## Geometry    0.879         \n## \n##                Factor1 Factor2\n## SS loadings      3.217   3.092\n## Proportion Var   0.402   0.387\n## Cumulative Var   0.402   0.789"},{"path":"exploratory-factor-analysis.html","id":"communality","chapter":"4 Exploratory Factor Analysis","heading":"4.2.3.3 Communality","text":"Communality represents proportion variance observed variable (indicator) accounted underlying factors. words, communality measures much variability variable explained common factors extracted factor analysis. Communality value 0 1, 0 indicates none variance variable explained common factors (.e., unique variable ), 1 suggests variance explained common factors.Interpretation: higher communality value variable indicates larger proportion variance can attributed underlying factors. Variables high communality values well-represented factors strong associations underlying dimensions.\nCommunality can also calculated sum squared factor loadings variable. words, quantifies much variability variable can attributed factors, evidenced squares factor loadings.\nuse Rcode find communality.results example, high communality values, ranging approximately 0.720 0.834, indicate substantial portion variability variables can attributed underlying factors. suggests factors effective capturing essence variables, making suitable representing latent dimensions uncovered factor analysis. High communality values reflect strong association variables common factors, contributing straightforward interpretable factor structure.","code":"\napply(fit$loadings^2,1,sum)##    English      Hindi      Maths     French    Physics     Poetry Statistics \n##  0.7695653  0.7729351  0.8263179  0.7207453  0.8125261  0.8340212  0.7992973 \n##   Geometry \n##  0.7739642"},{"path":"exploratory-factor-analysis.html","id":"uniqueness","chapter":"4 Exploratory Factor Analysis","heading":"4.2.3.4 Uniqueness","text":"uniqueness (also known “specific variance” “unique variance”) concept represents variance observed variable explained latent variables model. Uniqueness value typically 0 1, 0 indicates variance variable explained common factors, 1 implies none variance accounted factors.Interpretation: low uniqueness value suggests variable highly related underlying factors model. case, variance variable explained common factors, indicating variable good indicator latent constructs.uniqueness values variables obtained reveal proportion variance specific variable shared common factors. instance, high uniqueness values, like approximately 27.93% “French,” indicate substantial part variance “French” unique language, highlighting distinct characteristics within factor analysis model. uniqueness values assist researchers determining extent variables idiosyncratic can essential identifying variables may fit well factor structure gaining insights individual features variable. Overall low values uniqueness indicate significant portion variance variables accounted latent factors, can positive sign quality factor analysis.can also calculate correlation variables using Rcode , insights required.","code":"\nfit$uniquenesses##    English      Hindi      Maths     French    Physics     Poetry Statistics \n##  0.2304343  0.2270650  0.1736824  0.2792547  0.1874740  0.1659783  0.2007031 \n##   Geometry \n##  0.2260363\ncorrel <- round(fit$correlation, 3) # round correlation to 3 digits\ncorrel##            English  Hindi  Maths French Physics Poetry Statistics Geometry\n## English      1.000  0.744  0.008  0.786  -0.027  0.794      0.010    0.041\n## Hindi        0.744  1.000 -0.017  0.740  -0.035  0.824     -0.029   -0.009\n## Maths        0.008 -0.017  1.000  0.003   0.826 -0.074      0.795    0.810\n## French       0.786  0.740  0.003  1.000   0.003  0.749      0.038   -0.004\n## Physics     -0.027 -0.035  0.826  0.003   1.000 -0.080      0.815    0.772\n## Poetry       0.794  0.824 -0.074  0.749  -0.080  1.000     -0.061   -0.015\n## Statistics   0.010 -0.029  0.795  0.038   0.815 -0.061      1.000    0.796\n## Geometry     0.041 -0.009  0.810 -0.004   0.772 -0.015      0.796    1.000"},{"path":"exploratory-factor-analysis.html","id":"draw-diagrams","chapter":"4 Exploratory Factor Analysis","heading":"4.2.3.5 Draw diagrams","text":"Use R code draw diagram visualize factor analysis. Lets simply visualize variables correlation. code good visualize number variables .can save diagrams high resolution using codes :\nNow draw factor diagram using code :","code":"\n# install.packages(\"PerformanceAnalytics\")\nlibrary(\"PerformanceAnalytics\")\nchart<-chart.Correlation(score, histogram=TRUE, pch=19)\nloads <- fit$loadings\nfa_diag<-fa.diagram(loads)\n#saving visulaization plot \npng(\"Plot.png\", res = 72, width = 1000, height = 1000, pointsize = 25)\nPerformanceAnalytics::chart.Correlation(score, histogram=TRUE, pch=19)\ndev.off()  \n\n#saving factor analysis diagram\npng(\"Plot.png\", res = 72, width = 1000, height = 1000, pointsize = 25)\nloads <- fit$loadings\nfa_diag<-fa.diagram(loads)\ndev.off()  \n\n## change to required resolution in the parameter res ="},{"path":"references.html","id":"references","chapter":"5 References","heading":"5 References","text":"","code":""}]

<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Principal Component Analysis (PCA) | MULTIVARIATE DATA ANALYSIS TOOLS FOR AGRICULTURAL RESEARCH</title>
<meta name="author" content="Dr. PRATHEESH P GOPINATH">
<meta name="description" content="3.1 What is PCA? Principal component analysis (PCA) is a technique that transforms high-dimensions data into lower-dimensions while retaining as much information as possible. PCA was invented in...">
<meta name="generator" content="bookdown 0.36 with bs4_book()">
<meta property="og:title" content="Chapter 3 Principal Component Analysis (PCA) | MULTIVARIATE DATA ANALYSIS TOOLS FOR AGRICULTURAL RESEARCH">
<meta property="og:type" content="book">
<meta property="og:image" content="/images/cover.PNG">
<meta property="og:description" content="3.1 What is PCA? Principal component analysis (PCA) is a technique that transforms high-dimensions data into lower-dimensions while retaining as much information as possible. PCA was invented in...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Principal Component Analysis (PCA) | MULTIVARIATE DATA ANALYSIS TOOLS FOR AGRICULTURAL RESEARCH">
<meta name="twitter:description" content="3.1 What is PCA? Principal component analysis (PCA) is a technique that transforms high-dimensions data into lower-dimensions while retaining as much information as possible. PCA was invented in...">
<meta name="twitter:image" content="/images/cover.PNG">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.1/transition.js"></script><script src="libs/bs3compat-0.5.1/tabs.js"></script><script src="libs/bs3compat-0.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script><link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="libs/datatables-binding-0.29/datatables.js"></script><link href="libs/dt-core-1.13.4/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="libs/dt-core-1.13.4/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="libs/dt-core-1.13.4/js/jquery.dataTables.min.js"></script><link href="libs/dt-ext-buttons-1.13.4/css/buttons.dataTables.min.css" rel="stylesheet">
<script src="libs/dt-ext-buttons-1.13.4/js/dataTables.buttons.min.js"></script><script src="libs/dt-ext-buttons-1.13.4/js/buttons.html5.min.js"></script><script src="libs/dt-ext-buttons-1.13.4/js/buttons.colVis.min.js"></script><script src="libs/dt-ext-buttons-1.13.4/js/buttons.print.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">MULTIVARIATE DATA ANALYSIS TOOLS FOR AGRICULTURAL RESEARCH</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="r-and-r-studio.html"><span class="header-section-number">1</span> R and R studio</a></li>
<li><a class="" href="import.html"><span class="header-section-number">2</span> Importing data files in R</a></li>
<li><a class="active" href="principal-component-analysis-pca.html"><span class="header-section-number">3</span> Principal Component Analysis (PCA)</a></li>
<li><a class="" href="exploratory-factor-analysis.html"><span class="header-section-number">4</span> Exploratory Factor Analysis</a></li>
<li><a class="" href="references.html"><span class="header-section-number">5</span> References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="principal-component-analysis-pca" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Principal Component Analysis (PCA)<a class="anchor" aria-label="anchor" href="#principal-component-analysis-pca"><i class="fas fa-link"></i></a>
</h1>
<div id="what-is-pca" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> What is PCA?<a class="anchor" aria-label="anchor" href="#what-is-pca"><i class="fas fa-link"></i></a>
</h2>
<p>Principal component analysis (PCA) is a technique that transforms high-dimensions data into lower-dimensions while retaining as much information as possible. PCA was invented in 1909 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s.<span class="citation">(<a href="references.html#ref-johnson2014">Johnson and Wichern 2014</a>)</span></p>
<p>Drawing meaningful inferences from high-dimensional data can be challenging, as humans naturally excel at visualizing and comprehending information in two dimensions. PCA, a powerful technique, aids in transforming multi-dimensional data into a more manageable form by reducing its dimensionality. This simplification facilitates easier visualization and analysis, ultimately enhancing our ability to extract valuable insights from complex datasets.</p>
<p>PCA is a valuable tool in social science and agricultural research. It works by transforming multi-dimensional data into a lower-dimensional space while retaining as much variance in the data as possible. In essence, PCA identifies the most significant dimensions or “principal components” of the data, effectively reducing its complexity.
The idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible.</p>
</div>
<div id="when-to-use-pca" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> When to Use PCA<a class="anchor" aria-label="anchor" href="#when-to-use-pca"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Dimensionality Reduction</strong>: Use PCA when you have a high-dimensional dataset with many features (variables) and you want to reduce its dimensionality. This can help in cases where you have too many variables to work with efficiently.</p>
<p><strong>Data Visualization</strong>: PCA is effective when you need to visualize high-dimensional data. By projecting data onto a lower-dimensional space, you can create scatter plots, heatmaps, or other visualizations that are easier to interpret.</p>
<p><strong>Minimum data set</strong>: Principal components can be used to eliminate some data sets and identify a minimum data set for further experimentation.</p>
<p><strong>Noise Reduction</strong>: If your dataset contains noisy or redundant features, PCA can help by capturing the most important information and eliminating the less relevant components.</p>
<p><strong>Multicollinearity</strong>: When your dataset has multicollinearity issues (high correlations between variables), PCA can help reduce these interdependencies, making models more stable and interpretable.</p>
<p>Sample size (n) should be at least equal to number of dimensions (n ≥ p)</p>
</div>
<div id="when-not-to-use-pca" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> When Not to Use PCA<a class="anchor" aria-label="anchor" href="#when-not-to-use-pca"><i class="fas fa-link"></i></a>
</h2>
<p>When the <strong>sample size</strong> is less than number of dimensions (n &lt; p)</p>
<p><strong>Non-Linear Relationships</strong>: PCA is based on linear transformations and may not be effective when your data contains complex non-linear relationships. In such cases, techniques like kernel PCA or other non-linear dimensionality reduction methods might be more appropriate.</p>
<p><strong>Small Dimensionality</strong>: If you already have a low-dimensional dataset with only a few important variables, applying PCA might not provide significant benefits and could even lead to information loss.</p>
<p><strong>Loss of Variability Information</strong>: PCA aims to maximize variance capture, which may not be desirable in some cases. If preserving other characteristics of the data is more important (e.g., categorical information), other dimensionality reduction techniques should be considered.</p>
</div>
<div id="how-pca-is-done" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> How PCA is done<a class="anchor" aria-label="anchor" href="#how-pca-is-done"><i class="fas fa-link"></i></a>
</h2>
<p>In this training program, we are primarily focused on the practical application of PCA rather than delving into its theoretical aspects. Our aim is to explore when and how PCA can be effectively utilized and to understand how to interpret the results in a meaningful manner. But the usual procedure of performing is as follows for your information:-<br>
* Standardize the range of continuous initial variables<br>
* Compute the covariance matrix to identify correlations<br>
* Compute the eigen vectors and eigen values of the covariance matrix to identify the principal components<br>
* Create a feature vector to decide which principal components to keep<br>
* Recast the data along the principal components axes</p>
</div>
<div id="what-is-principal-component" class="section level2" number="3.5">
<h2>
<span class="header-section-number">3.5</span> What is Principal Component<a class="anchor" aria-label="anchor" href="#what-is-principal-component"><i class="fas fa-link"></i></a>
</h2>
<p>Principal components are new variables that are constructed as linear combinations of the initial variables. These combinations are done in such a way that the new variables (i.e. principal components) are uncorrelated and most of the information within the initial variables are included in the first components. So, the idea if 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on.</p>
<p>In the example scree plot shown below you can see 5 principal components of a 5-dimensional data and the corresponding variance explained.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:pcavar"></span>
<img src="images/PCA_img/pcavar.png" alt="Scree Plot: Principal components and percentage variance explained" width="70%"><p class="caption">
Figure 3.1: Scree Plot: Principal components and percentage variance explained
</p>
</div>
</div>
<div id="how-pca-constructs-the-principal-components" class="section level2" number="3.6">
<h2>
<span class="header-section-number">3.6</span> How PCA Constructs the Principal Components<a class="anchor" aria-label="anchor" href="#how-pca-constructs-the-principal-components"><i class="fas fa-link"></i></a>
</h2>
<p>Number of principal components are equal to the number of variables in the data, principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. For example, see the Figure 1.2 below, we can see the scatter plot of our assumed data set, can we guess the first principal component ? Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:pcagif"></span>
<img src="images/PCA_img/pca.gif" alt="Concept of PCA" width="100%"><p class="caption">
Figure 3.2: Concept of PCA
</p>
</div>
<p>These lines (PCs) were identified using linear algebra concepts Eigen vectors and eigen values which are calculated from the covariance matrix in order to determine the principal components of the data. We are not going much in to theoretical details.</p>
</div>
<div id="practical-example" class="section level2" number="3.7">
<h2>
<span class="header-section-number">3.7</span> PRACTICAL EXAMPLE<a class="anchor" aria-label="anchor" href="#practical-example"><i class="fas fa-link"></i></a>
</h2>
<p>We will be using Usarrests dataset in R to explain PCA in the coming sessions.The “USarrests” dataset in R is a built-in dataset that offers insights into crime and arrests across the 50 states of the United States in 1973. It comprises four key variables: murder rate, assault rate, the percentage of the population living in urban areas, and the rape rate.</p>
<p>You can View and download the datasets here</p>
<div class="datatables html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-c384c7773e970d7b7da5" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-c384c7773e970d7b7da5">{"x":{"filter":"none","vertical":false,"extensions":["Buttons"],"data":[["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois","Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania","Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"],[13.2,10,8.1,8.800000000000001,9,7.9,3.3,5.9,15.4,17.4,5.3,2.6,10.4,7.2,2.2,6,9.699999999999999,15.4,2.1,11.3,4.4,12.1,2.7,16.1,9,6,4.3,12.2,2.1,7.4,11.4,11.1,13,0.8,7.3,6.6,4.9,6.3,3.4,14.4,3.8,13.2,12.7,3.2,2.2,8.5,4,5.7,2.6,6.8],[236,263,294,190,276,204,110,238,335,211,46,120,249,113,56,115,109,249,83,300,149,255,72,259,178,109,102,252,57,159,285,254,337,45,120,151,159,106,174,279,86,188,201,120,48,156,145,81,53,161],[58,48,80,50,91,78,77,72,80,60,83,54,83,65,57,66,52,66,51,67,85,74,66,44,70,53,62,81,56,89,70,86,45,44,75,68,67,72,87,48,45,59,80,80,32,63,73,39,66,60],[21.2,44.5,31,19.5,40.6,38.7,11.1,15.8,31.9,25.8,20.2,14.2,24,21,11.3,18,16.3,22.2,7.8,27.8,16.3,35.1,14.9,17.1,28.2,16.4,16.5,46,9.5,18.8,32.1,26.1,16.1,7.3,21.4,20,29.3,14.9,8.300000000000001,22.5,12.8,26.9,25.5,22.9,11.2,20.7,26.2,9.300000000000001,10.8,15.6]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Murder<\/th>\n      <th>Assault<\/th>\n      <th>UrbanPop<\/th>\n      <th>Rape<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"Bfrtip","buttons":[{"extend":"csv","text":"Download CSV ⬇","filename":"data"}],"searching":false,"columnDefs":[{"className":"dt-right","targets":[1,2,3,4]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="analysis" class="section level2" number="3.8">
<h2>
<span class="header-section-number">3.8</span> Analysis<a class="anchor" aria-label="anchor" href="#analysis"><i class="fas fa-link"></i></a>
</h2>
<p>First prepare your data set and save it as a csv file. Then import the data set in to R. See chapter <a href="import.html#import">2</a> to know how to save a csv file and import it to R. You can directly use the code below to import dataset from your computer to R.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="st">"path to your file"</span>, row.names<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p>Please note the path copied from your system will be in the format <code>C:\Users\HP\Documents\</code>, so you should change it to this format <code>C:/Users/HP/Documents/</code> in R.</p>
<div class="figure" style="text-align: center">
<img src="images/PCA_img/dataset.png" alt="Data set after importing to R in Rstudio" width="70%"><p class="caption">
(#fig:data_imp)Data set after importing to R in Rstudio
</p>
</div>

<div class="rmdnote">
While importing csv file for PCA don’t forget to set first column as rownames. In the code above row.names = 1 will set the first column as rowname.
</div>
<p>We will be using the package <code>factoextra</code> for performing the analysis</p>
<p><strong>Install the required packages</strong>:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"factoextra"</span><span class="op">)</span>  </span></code></pre></div>
<p><strong>Follow below codes for PCA analysis</strong></p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sthda.com/english/rpkgs/factoextra">factoextra</a></span><span class="op">)</span></span>
<span><span class="co">#storing your dataset to the variable data</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="st">"csv/usarrests.csv"</span>, row.names<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#this will display the first 6 rows of your data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span>, n <span class="op">=</span> <span class="fl">6</span><span class="op">)</span></span></code></pre></div>
<pre><code>##            Murder Assault UrbanPop Rape
## Alabama      13.2     236       58 21.2
## Alaska       10.0     263       48 44.5
## Arizona       8.1     294       80 31.0
## Arkansas      8.8     190       50 19.5
## California    9.0     276       91 40.6
## Colorado      7.9     204       78 38.7</code></pre>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Do PCA using prcomp function in factoextra</span></span>
<span><span class="va">pca_res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">data</span>, scale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co"># scale = TRUE will standardise the variables (x-mean(x)/sd(x))</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">pca_res</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Importance of components:
##                           PC1    PC2     PC3     PC4
## Standard deviation     1.5749 0.9949 0.59713 0.41645
## Proportion of Variance 0.6201 0.2474 0.08914 0.04336
## Cumulative Proportion  0.6201 0.8675 0.95664 1.00000</code></pre>
</div>
<div id="deciding-on-the-number-of-pcs" class="section level2" number="3.9">
<h2>
<span class="header-section-number">3.9</span> Deciding on the number of PCs<a class="anchor" aria-label="anchor" href="#deciding-on-the-number-of-pcs"><i class="fas fa-link"></i></a>
</h2>
<div id="scree-plot" class="section level3" number="3.9.1">
<h3>
<span class="header-section-number">3.9.1</span> Scree Plot<a class="anchor" aria-label="anchor" href="#scree-plot"><i class="fas fa-link"></i></a>
</h3>
<p>A scree plot helps you decide how many principal components to retain in your PCA analysis. The choice of the number of components can vary depending on your specific goals, but it’s often based on a combination of statistical criteria, such as the explained variance and the elbow point, as well as domain knowledge and interpretability.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Visualize eigenvalues (scree plot)</span></span>
<span><span class="va">plot1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/eigenvalue.html">fviz_eig</a></span><span class="op">(</span><span class="va">pca_res</span>,addlabels <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">plot1</span></span></code></pre></div>
<div class="inline-figure"><img src="lecture_note_files/figure-html/PCA-anal1-1.png" width="672"></div>
<p><strong>Here are the key inferences you can make from this plot:</strong><br><strong>Explained Variance:</strong> The scree plot displays the proportion of total variance explained by each principal component. Inferences can be made by examining how much variance is explained by each component. The components on the left contribute the most to the variance, while those on the right contribute less.</p>
<p><strong>Elbow Point:</strong> Look for an “elbow” or point where the explained variance sharply decreases. This is often a good indicator of the number of principal components to retain. The point just before the explained variance starts to level off can be a suitable choice. It’s the point where adding more components doesn’t explain much additional variance.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Drawing an elbow point if needed</span></span>
<span><span class="va">pca.var</span> <span class="op">=</span><span class="va">pca_res</span><span class="op">$</span><span class="va">sdev</span> <span class="op">^</span><span class="fl">2</span></span>
<span><span class="va">var.ratio</span><span class="op">=</span><span class="va">pca.var</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">pca.var</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">var.ratio</span> , xlab<span class="op">=</span><span class="st">" Principal Component "</span>, ylab<span class="op">=</span><span class="st">" Proportion of</span></span>
<span><span class="st">Variance Explained "</span>, ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span> ,type<span class="op">=</span><span class="st">"b"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="lecture_note_files/figure-html/elbow-1.png" width="672"></div>
<p><strong>Cumulative Variance:</strong> You can also examine the cumulative explained variance. The scree plot may show a cumulative curve that increases as more components are added. You can look for the point where the cumulative variance reaches a satisfactory level (e.g., 70%, 80%, 90%) to determine the number of components to retain.</p>
<p><strong>Interpretability:</strong> Consider the interpretability of the components. Sometimes, you might choose to retain more components even if they explain less variance because they have meaningful interpretations in your context.</p>
<p><strong>Domain Knowledge:</strong> Always consider the domain or subject matter knowledge when deciding on the number of components to retain. Sometimes, the context of your analysis may dictate the number of components that are practically meaningful.</p>
</div>
<div id="eigen-values" class="section level3" number="3.9.2">
<h3>
<span class="header-section-number">3.9.2</span> Eigen Values<a class="anchor" aria-label="anchor" href="#eigen-values"><i class="fas fa-link"></i></a>
</h3>
<p>The “Eigenvalues” represent the variance explained by each principal component (PC) in a PCA. Percentage variance is the ratio of the eigen value of the principal component to the sum of the eigen values of all PCs. It is important to note that first two PCs should atleast explain 80% variance of the data.</p>
<p>Eigenvalues can be used to determine the number of principal components to retain after PCA (Kaiser 1961):</p>
<p>An eigenvalue &gt; 1 indicates that PCs account for more variance than accounted by one of the original variables in standardized data. This is commonly used as a cutoff point for which PCs are retained. This holds true only when the data are standardized.</p>
<p>You can also select the number of principal components in a Principal Component Analysis (PCA) based on a desired level of explained variance. For instance, if you want to retain 80% of the total variance explained, you can choose the number of components that achieves that level.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Eigenvalues</span></span>
<span><span class="va">eig.val</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/eigenvalue.html">get_eigenvalue</a></span><span class="op">(</span><span class="va">pca_res</span><span class="op">)</span></span>
<span><span class="va">eig.val</span></span></code></pre></div>
<pre><code>##       eigenvalue variance.percent cumulative.variance.percent
## Dim.1  2.4802416        62.006039                    62.00604
## Dim.2  0.9897652        24.744129                    86.75017
## Dim.3  0.3565632         8.914080                    95.66425
## Dim.4  0.1734301         4.335752                   100.00000</code></pre>

<div class="rmdnote">
From the scree plot and eigen values it is clear that first two PCs combined together explains 86.75% of variance of the data. So we will keep only two PCs in the further analysis.
</div>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Now we will take the PCA results keeping first two pricipal components only</span></span>
<span><span class="va">res.pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">USArrests</span>, scale <span class="op">=</span> <span class="cn">TRUE</span>, rank <span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#here rank =2 will keep only two PCs</span></span></code></pre></div>
</div>
</div>
<div id="accessing-the-pca-results" class="section level2" number="3.10">
<h2>
<span class="header-section-number">3.10</span> Accessing the PCA results<a class="anchor" aria-label="anchor" href="#accessing-the-pca-results"><i class="fas fa-link"></i></a>
</h2>
<p>Please note that in the results discussed further Dim1 , Dim2 etc denotes Principal Component1(PC1), PC2 etc respectively. We will now discuss the terms in PCA and how it can be interpreted to get meaningful insights.</p>
<div id="pc-loadings" class="section level3" number="3.10.1">
<h3>
<span class="header-section-number">3.10.1</span> PC Loadings<a class="anchor" aria-label="anchor" href="#pc-loadings"><i class="fas fa-link"></i></a>
</h3>
<p>Loadings are coefficients in linear combination predicting a variable by the (standardized) components. Loadings represent the weights assigned to each original variable in the linear combination that forms a principal component. These weights indicate the importance of each variable in creating the component. In more statistical language loadings are eigenvectors scaled by the square roots of the respective eigenvalues.</p>
<p>Positive loadings indicate a positive relationship between the variable and the component, suggesting that an increase in the variable is associated with an increase in the component’s value. Negative loadings indicate a negative relationship, meaning that an increase in the variable corresponds to a decrease in the component’s value. The magnitude of the loading reflects the strength of the relationship. Larger loadings indicate that the variable has a more substantial impact on the component. Loadings are typically standardized to have a mean of 0 and a standard deviation of 1, ensuring that variables with different scales are directly comparable.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#getting PC loadings</span></span>
<span><span class="va">res.pca</span><span class="op">$</span><span class="va">rotation</span></span></code></pre></div>
<pre><code>##                 PC1        PC2
## Murder   -0.5358995 -0.4181809
## Assault  -0.5831836 -0.1879856
## UrbanPop -0.2781909  0.8728062
## Rape     -0.5434321  0.1673186</code></pre>

<div class="rmdnote">
<p>The first loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight on UrbanPop. Hence this component roughly corresponds to a measure of overall rates of serious crimes. All the loadings were negative so we can assume that states scoring lesser values for PC1 will have higher crime rate.</p>
The second loading vector places most of its weight on UrbanPop and much less weight on the other three features. Hence, this component roughly corresponds to the level of urbanization of the state. And the loading is positive this indicates the states with higher values in PC2 have greater urbanization level.
</div>
</div>
<div id="variable-coordinates" class="section level3" number="3.10.2">
<h3>
<span class="header-section-number">3.10.2</span> Variable coordinates<a class="anchor" aria-label="anchor" href="#variable-coordinates"><i class="fas fa-link"></i></a>
</h3>
<p>These coordinates provides insights into the relative positions of variables in the PCA space and how they relate to the principal components geometrically. Absolute value of these measures gives strength of association of each variable with particular PCs. You can avoid these in your final tables, while presenting the results. These coordinates will be already represented in the biplots.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Results for Variables</span></span>
<span><span class="va">res.var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/get_pca.html">get_pca_var</a></span><span class="op">(</span><span class="va">res.pca</span><span class="op">)</span></span>
<span><span class="va">res.var</span><span class="op">$</span><span class="va">coord</span>          <span class="co"># Coordinates  </span></span></code></pre></div>
<pre><code>##               Dim.1      Dim.2      Dim.3       Dim.4
## Murder   -0.8439764 -0.4160354 -0.3200012 -0.17415116
## Assault  -0.9184432 -0.1870211 -0.3482359 -0.07828649
## UrbanPop -0.4381168  0.8683282 -0.1661159  0.36347960
## Rape     -0.8558394  0.1664602 -0.3244991  0.06967974</code></pre>
</div>
<div id="variable-contributions" class="section level3" number="3.10.3">
<h3>
<span class="header-section-number">3.10.3</span> Variable contributions<a class="anchor" aria-label="anchor" href="#variable-contributions"><i class="fas fa-link"></i></a>
</h3>
<p>These measures provides the percentage contributions of each variable to each principal component. It indicates the proportion of variance explained by each variable in each principal component. Higher values suggest that a variable has a more substantial influence on the formation of the respective component. It helps identify variables that contribute significantly to the variance explained by each component and is useful for variable selection and interpretation.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res.var</span><span class="op">$</span><span class="va">contrib</span>        <span class="co"># Contributions to the PCs  </span></span></code></pre></div>
<pre><code>##              Dim.1     Dim.2     Dim.3     Dim.4
## Murder   28.718825 17.487524 28.718825 17.487524
## Assault  34.010315  3.533859 34.010315  3.533859
## UrbanPop  7.739016 76.179065  7.739016 76.179065
## Rape     29.531844  2.799553 29.531844  2.799553</code></pre>
<p>These contributions reflect the same idea what the loadings convey. You can see higher contribution of Assault, Murder, and Rape on PC1 and higher contribution of urban population in PC2.</p>
</div>
</div>
<div id="cos2-representation" class="section level2" number="3.11">
<h2>
<span class="header-section-number">3.11</span> cos2 representation<a class="anchor" aria-label="anchor" href="#cos2-representation"><i class="fas fa-link"></i></a>
</h2>
<p>Cosine Squared (Cos²) Values: cos2 values provides the cosine squared values for each variable with respect to each principal component. Cosine squared values are a measure of how much of the variable’s variance is explained by the corresponding principal component. The cos2 values typically range from 0 to 1. A value of 1 indicates that the variable is perfectly aligned with the principal component, meaning that the variable’s entire variance is explained by that component. A value of 0 indicates that the variable is orthogonal with the component.<br>
Higher cos2 values indicate that the variable is well-represented by the principal component. These variables contribute significantly to the explanation of variance along that component. Lower values suggest that the variable has less alignment with the component and is less influential in explaining the variance.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res.var</span><span class="op">$</span><span class="va">cos2</span>           <span class="co"># Quality of representation </span></span></code></pre></div>
<pre><code>##              Dim.1     Dim.2      Dim.3       Dim.4
## Murder   0.7122962 0.1730854 0.10240075 0.030328628
## Assault  0.8435380 0.0349769 0.12126826 0.006128774
## UrbanPop 0.1919463 0.7539938 0.02759448 0.132117419
## Rape     0.7324611 0.0277090 0.10529968 0.004855266</code></pre>
<p>You can exclude these values in the final table while presenting the results.</p>
<div id="measures-of-indviduals" class="section level3" number="3.11.1">
<h3>
<span class="header-section-number">3.11.1</span> Measures of Indviduals<a class="anchor" aria-label="anchor" href="#measures-of-indviduals"><i class="fas fa-link"></i></a>
</h3>
<p>Similar measures are calculated for individuals also. Those individuals with higher values on PCs can be identified and insights can be drawn based on that.<br>
In our example states with high negative values for PC1 has high crime rate and states with high values in PC2 has higher level of urbanization.<br>
principal components scores vector for all 50 states(indviduals) can be viewed using following code.</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># principal components scores vector for all 50 states(indviduals)</span></span>
<span><span class="va">res.pca</span><span class="op">$</span><span class="va">x</span></span></code></pre></div>
<pre><code>##                        PC1         PC2
## Alabama        -0.97566045 -1.12200121
## Alaska         -1.93053788 -1.06242692
## Arizona        -1.74544285  0.73845954
## Arkansas        0.13999894 -1.10854226
## California     -2.49861285  1.52742672
## Colorado       -1.49934074  0.97762966
## Connecticut     1.34499236  1.07798362
## Delaware       -0.04722981  0.32208890
## Florida        -2.98275967 -0.03883425
## Georgia        -1.62280742 -1.26608838
## Hawaii          0.90348448  1.55467609
## Idaho           1.62331903 -0.20885253
## Illinois       -1.36505197  0.67498834
## Indiana         0.50038122  0.15003926
## Iowa            2.23099579  0.10300828
## Kansas          0.78887206  0.26744941
## Kentucky        0.74331256 -0.94880748
## Louisiana      -1.54909076 -0.86230011
## Maine           2.37274014 -0.37260865
## Maryland       -1.74564663 -0.42335704
## Massachusetts   0.48128007  1.45967706
## Michigan       -2.08725025  0.15383500
## Minnesota       1.67566951  0.62590670
## Mississippi    -0.98647919 -2.36973712
## Missouri       -0.68978426  0.26070794
## Montana         1.17353751 -0.53147851
## Nebraska        1.25291625  0.19200440
## Nevada         -2.84550542  0.76780502
## New Hampshire   2.35995585  0.01790055
## New Jersey     -0.17974128  1.43493745
## New Mexico     -1.96012351 -0.14141308
## New York       -1.66566662  0.81491072
## North Carolina -1.11208808 -2.20561081
## North Dakota    2.96215223 -0.59309738
## Ohio            0.22369436  0.73477837
## Oklahoma        0.30864928  0.28496113
## Oregon         -0.05852787  0.53596999
## Pennsylvania    0.87948680  0.56536050
## Rhode Island    0.85509072  1.47698328
## South Carolina -1.30744986 -1.91397297
## South Dakota    1.96779669 -0.81506822
## Tennessee      -0.98969377 -0.85160534
## Texas          -1.34151838  0.40833518
## Utah            0.54503180  1.45671524
## Vermont         2.77325613 -1.38819435
## Virginia        0.09536670 -0.19772785
## Washington      0.21472339  0.96037394
## West Virginia   2.08739306 -1.41052627
## Wisconsin       2.05881199  0.60512507
## Wyoming         0.62310061 -0.31778662</code></pre>

<div class="rmdnote">
<p>California, Nevada and Florida, have high crime rates as they have large negative scores on the first component, while states like North Dakota, with positive scores on the first component, have low crime rates.</p>
<p>California also has a high score on the second component, indicating a high level of urbanization, while the opposite is true for states like Mississippi.</p>
States close to zero on both components, such as Indiana, have approximately average levels of both crime and urbanization.
</div>
<p>You can find the contributions, cos2 and coordinates of the individuals, if an in depth analysis is required for your study, using the code below.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res.ind</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/get_pca.html">get_pca_ind</a></span><span class="op">(</span><span class="va">res.pca</span><span class="op">)</span></span>
<span><span class="va">res.ind</span><span class="op">$</span><span class="va">coord</span>          <span class="co"># Coordinates</span></span>
<span><span class="va">res.ind</span><span class="op">$</span><span class="va">contrib</span>        <span class="co"># Contributions to the PCs</span></span>
<span><span class="va">res.ind</span><span class="op">$</span><span class="va">cos2</span>           <span class="co"># Quality of representation </span></span></code></pre></div>
</div>
</div>
<div id="biplot-indviduals" class="section level2" number="3.12">
<h2>
<span class="header-section-number">3.12</span> Biplot (Indviduals)<a class="anchor" aria-label="anchor" href="#biplot-indviduals"><i class="fas fa-link"></i></a>
</h2>
<p>A “Biplot of Individuals” is a graphical representation used in Principal Component Analysis (PCA) and other multivariate statistical methods to visualize how individual data points or observations are related to the underlying patterns and variables in a dataset. By examining the biplot, analysts and researchers can gain insights into various aspects of the data, including clustering patterns among individual observations.</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Graph of individuals. Individuals with a similar profile are grouped together (Biplot)</span></span>
<span><span class="va">biplot1</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_pca.html">fviz_pca_ind</a></span><span class="op">(</span><span class="va">res.pca</span>,</span>
<span>             col.ind <span class="op">=</span> <span class="st">"cos2"</span>, <span class="co"># Color by the quality of representation</span></span>
<span>             gradient.cols <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"#00AFBB"</span>, <span class="st">"#E7B800"</span>, <span class="st">"#FC4E07"</span><span class="op">)</span>,</span>
<span>             repel <span class="op">=</span> <span class="cn">TRUE</span>     <span class="co"># Avoid text overlapping</span></span>
<span>             <span class="op">)</span></span>
<span><span class="va">biplot1</span></span></code></pre></div>
<p><img src="lecture_note_files/figure-html/PCA-anal2-1.png" width="672"><strong>Interpretation of the Biplot of indviduals</strong></p>
<p><strong>Individuals’ Position:</strong> Each point in the biplot represents an individual observation from your dataset. The positions of the points in the plot show how each individual relates to the principal components. Individuals closer to each other on the plot are more similar in terms of their relationships with the principal components.</p>
<p><strong>Color Coding:</strong> The points representing individuals may be color-coded based on the “cos2” values, which indicate the quality of representation of each individual on the plot. Higher “cos2” values indicate that an individual’s position on the plot is more strongly associated with the principal components.</p>
<p><strong>“cos2” in the Biplot:</strong><br>
In the context of a PCA biplot, “cos2” represents the square of the cosine of the angle between the individual’s position (point) and the variable’s arrow on the plot. It quantifies the quality of representation of an individual in the plot.</p>
<ul>
<li><p>High “cos2” values suggest that the individual’s position in the plot is well-represented by the variables and principal components.</p></li>
<li><p>Low “cos2” values suggest that the individual’s position may not be well-represented or may be noisy in the plot</p></li>
</ul>
</div>
<div id="biplot-variables" class="section level2" number="3.13">
<h2>
<span class="header-section-number">3.13</span> Biplot (variables)<a class="anchor" aria-label="anchor" href="#biplot-variables"><i class="fas fa-link"></i></a>
</h2>
<p>In this biplot, variables are represented as arrows or vectors, and the plot allows for the examination of how these variables are associated with each other and with the principal components.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Graph of variables. </span></span>
<span><span class="va">biplot2</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_pca.html">fviz_pca_var</a></span><span class="op">(</span><span class="va">res.pca</span>,</span>
<span>             col.var <span class="op">=</span> <span class="st">"contrib"</span>, <span class="co"># Color by contributions to the PC</span></span>
<span>             gradient.cols <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"#00AFBB"</span>, <span class="st">"#E7B800"</span>, <span class="st">"#FC4E07"</span><span class="op">)</span>,</span>
<span>             repel <span class="op">=</span> <span class="cn">TRUE</span>     <span class="co"># Avoid text overlapping</span></span>
<span>             <span class="op">)</span></span>
<span><span class="va">biplot2</span></span></code></pre></div>
<p><img src="lecture_note_files/figure-html/PCA-anal3-1.png" width="672"><strong>Interpretation of the PCA Variable BiPlot:</strong></p>
<p><strong>Variable Positions:</strong> Each point in the variable plot represents a variable from your dataset. The positions of the points in the plot show how each variable relates to the principal components. Variables closer to each other on the plot are more similar in terms of their relationships with the principal components.</p>
<p><strong>Direction of Arrows:</strong> Arrows in the variable plot represent the variables and their contribution to the principal components. The direction of the arrows indicates how each variable is associated with each principal component. Variables pointing in the same direction are positively correlated with the corresponding component, while those pointing in opposite directions are negatively correlated. This helps you understand which variables move in the same direction as the component and which move in the opposite direction.</p>
<p><strong>Arrow Length:</strong> The length of the arrows indicates the strength of the relationship between variables and principal components. Longer arrows represent variables with a higher contribution to the component.</p>
<p><strong>Color Coding:</strong> The points representing variables may be color-coded based on their contributions to the principal components. Higher contributions are typically associated with a darker color.</p>
</div>
<div id="biplot-indviduals-and-variables" class="section level2" number="3.14">
<h2>
<span class="header-section-number">3.14</span> Biplot (Indviduals and variables)<a class="anchor" aria-label="anchor" href="#biplot-indviduals-and-variables"><i class="fas fa-link"></i></a>
</h2>
<p>This biplot combines individual data points represented as points in the plot with variables represented as arrows or vectors. It provides a powerful way to assess the relationships between individual observations and variables, offering insights into patterns, clusters, and associations within the data. By examining the positions of individuals and the directions of variable vectors, analysts can gain a holistic understanding of the data’s underlying structure, making it a valuable tool for dimensionality reduction and data exploration.</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Biplot of individuals and variables</span></span>
<span><span class="va">biplot3</span><span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_pca.html">fviz_pca_biplot</a></span><span class="op">(</span><span class="va">res.pca</span>, repel <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                col.var <span class="op">=</span> <span class="st">"#2E9FDF"</span>, <span class="co"># Variables color</span></span>
<span>                col.ind <span class="op">=</span> <span class="st">"#696969"</span>  <span class="co"># Individuals color</span></span>
<span>                <span class="op">)</span></span>
<span><span class="va">biplot3</span></span></code></pre></div>
<p><img src="lecture_note_files/figure-html/PCA-anal4-1.png" width="672"><strong>Interpretation of the PCA BiPlot (Individuals and Variables):</strong></p>
<p><strong>Distance from the Origin (Center):</strong> The distance of an individual point from the origin (center) of the plot represents its contribution to the explained variance by the principal components. Points that are further from the origin have a stronger influence on the principal components, while points closer to the origin have a weaker influence. In other words, individuals further from the origin are better represented by the principal components.</p>
<p><strong>Projection onto Arrows:</strong> The position of an individual point relative to the arrows tells you how that individual is influenced by the variables. If an individual point is projected close to the tip of an arrow, it indicates that this individual’s profile is strongly influenced by that particular variable represented by the arrow.</p>
<p><strong>Alignment of Points and Arrows:</strong> When the direction of arrows and the position of individuals align, it suggests that variables and individuals are positively correlated with the corresponding principal component. In other words, the variables that point in the same direction as the individual points contribute positively to the principal component.</p>
<p><strong>Opposite Direction:</strong> If an individual point and an arrow are in opposite directions, it indicates a negative correlation between the individual and the variable, implying that the variable contributes negatively to the principal component for that individual.</p>
<p>In summary, the relationship between the position of individuals and the direction of arrows in a PCA biplot provides insights into how individual observations relate to the variables and the principal components. The biplot helps you understand the strength and direction of these relationships, allowing you to identify which variables influence which components and how individuals are affected by these variables.</p>

<div class="rmdnote">
<p>Above biplot shows that 50 states mapped according to the 2 principal components. The vectors of the PCA for 4 variables are also plotted.</p>
<p>The states California, Nevada and Florida,are seen towards the extreme left, indicating a high negative value for PC1. i.e. these states have high crime rates.While states like North Dakota seen towards right have high positive scores on the first component, indicative of low crime rates.</p>
Also you can see California, New jersey etc in the top side indicating high values for PC2 suggesting increased uraban population. States close to zero on both components, such as Indiana, Virginia, oklahoma etc have approximately average levels of both crime and urbanization.
</div>
<p>In general we can divide the quadrants in the biplot of this example as follows:</p>
<ul>
<li>States in Quadrant I towards right: Low crime rate and high urbanization<br>
</li>
<li>States in Quadrant II towards left: High crime rate and high urbanization<br>
</li>
<li>States in Quadrant III towards bottom left: High crime rate and low urbanization<br>
</li>
<li>States in Quadrant IV towards bottom right: Low crime rate and low urbanization<br>
</li>
<li>Towards center: States with moderate crime rate and urbanization</li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:quadrant"></span>
<img src="images/PCA_img/quadrant.png" alt="Quadrants in biplot" width="30%"><p class="caption">
Figure 3.3: Quadrants in biplot
</p>
</div>
<p>Using the code below you can save the plots in required resolution. For changing resolution change the value of <code>res</code> in the code below.</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Save the scree plot as a PNG file</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/grDevices/png.html">png</a></span><span class="op">(</span><span class="st">"eigenvalues_plot.png"</span>, width <span class="op">=</span> <span class="fl">800</span>, height <span class="op">=</span> <span class="fl">600</span>, units <span class="op">=</span> <span class="st">"px"</span>, res <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span>  <span class="va">plot1</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/grDevices/dev.html">dev.off</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># Close the PNG device  </span></span>
<span></span>
<span>  <span class="co"># Save the biplot of indviduals a PNG file</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/grDevices/png.html">png</a></span><span class="op">(</span><span class="st">"bi_plot.png"</span>, width <span class="op">=</span> <span class="fl">800</span>, height <span class="op">=</span> <span class="fl">600</span>, units <span class="op">=</span> <span class="st">"px"</span>, res <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span>  <span class="va">biplot1</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/grDevices/dev.html">dev.off</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># Close the PNG device  </span></span>
<span>  </span>
<span>  <span class="co"># Save the biplot of variables a PNG file</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/grDevices/png.html">png</a></span><span class="op">(</span><span class="st">"bi_plot2.png"</span>, width <span class="op">=</span> <span class="fl">800</span>, height <span class="op">=</span> <span class="fl">600</span>, units <span class="op">=</span> <span class="st">"px"</span>, res <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span>  <span class="va">biplot2</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/grDevices/dev.html">dev.off</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># Close the PNG device  </span></span>
<span>  </span>
<span>  <span class="co"># Save the biplot of indviduals and variables a PNG file</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/grDevices/png.html">png</a></span><span class="op">(</span><span class="st">"bi_plot3.png"</span>, width <span class="op">=</span> <span class="fl">800</span>, height <span class="op">=</span> <span class="fl">600</span>, units <span class="op">=</span> <span class="st">"px"</span>, res <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span>  <span class="va">biplot3</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/grDevices/dev.html">dev.off</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># Close the PNG device  </span></span></code></pre></div>
</div>
<div id="do-pca-in-our-inbuilt-app" class="section level2" number="3.15">
<h2>
<span class="header-section-number">3.15</span> Do PCA in our inbuilt app<a class="anchor" aria-label="anchor" href="#do-pca-in-our-inbuilt-app"><i class="fas fa-link"></i></a>
</h2>
<p>Good news!
We have added an app so that you can simply upload your csv here and get all these results by running in our cloud server!.</p>
<button id="toggle-button">Open</button>
<div id="embedded-url" style="display: none;">
  <iframe src="https://coaagstat.shinyapps.io/pca_analysis" width="100%" height="400px" data-external="1"></iframe>
</div>
<script>
var isOpen = false;
document.getElementById("toggle-button").addEventListener("click", function() {
  if (isOpen) {
    document.getElementById("embedded-url").style.display = "none";
    document.getElementById("toggle-button").textContent = "Open";
  } else {
    document.getElementById("embedded-url").style.display = "block";
    document.getElementById("toggle-button").textContent = "Close";
  }
  isOpen = !isOpen;
});
</script>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="import.html"><span class="header-section-number">2</span> Importing data files in R</a></div>
<div class="next"><a href="exploratory-factor-analysis.html"><span class="header-section-number">4</span> Exploratory Factor Analysis</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#principal-component-analysis-pca"><span class="header-section-number">3</span> Principal Component Analysis (PCA)</a></li>
<li><a class="nav-link" href="#what-is-pca"><span class="header-section-number">3.1</span> What is PCA?</a></li>
<li><a class="nav-link" href="#when-to-use-pca"><span class="header-section-number">3.2</span> When to Use PCA</a></li>
<li><a class="nav-link" href="#when-not-to-use-pca"><span class="header-section-number">3.3</span> When Not to Use PCA</a></li>
<li><a class="nav-link" href="#how-pca-is-done"><span class="header-section-number">3.4</span> How PCA is done</a></li>
<li><a class="nav-link" href="#what-is-principal-component"><span class="header-section-number">3.5</span> What is Principal Component</a></li>
<li><a class="nav-link" href="#how-pca-constructs-the-principal-components"><span class="header-section-number">3.6</span> How PCA Constructs the Principal Components</a></li>
<li><a class="nav-link" href="#practical-example"><span class="header-section-number">3.7</span> PRACTICAL EXAMPLE</a></li>
<li><a class="nav-link" href="#analysis"><span class="header-section-number">3.8</span> Analysis</a></li>
<li>
<a class="nav-link" href="#deciding-on-the-number-of-pcs"><span class="header-section-number">3.9</span> Deciding on the number of PCs</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#scree-plot"><span class="header-section-number">3.9.1</span> Scree Plot</a></li>
<li><a class="nav-link" href="#eigen-values"><span class="header-section-number">3.9.2</span> Eigen Values</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#accessing-the-pca-results"><span class="header-section-number">3.10</span> Accessing the PCA results</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#pc-loadings"><span class="header-section-number">3.10.1</span> PC Loadings</a></li>
<li><a class="nav-link" href="#variable-coordinates"><span class="header-section-number">3.10.2</span> Variable coordinates</a></li>
<li><a class="nav-link" href="#variable-contributions"><span class="header-section-number">3.10.3</span> Variable contributions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#cos2-representation"><span class="header-section-number">3.11</span> cos2 representation</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#measures-of-indviduals"><span class="header-section-number">3.11.1</span> Measures of Indviduals</a></li></ul>
</li>
<li><a class="nav-link" href="#biplot-indviduals"><span class="header-section-number">3.12</span> Biplot (Indviduals)</a></li>
<li><a class="nav-link" href="#biplot-variables"><span class="header-section-number">3.13</span> Biplot (variables)</a></li>
<li><a class="nav-link" href="#biplot-indviduals-and-variables"><span class="header-section-number">3.14</span> Biplot (Indviduals and variables)</a></li>
<li><a class="nav-link" href="#do-pca-in-our-inbuilt-app"><span class="header-section-number">3.15</span> Do PCA in our inbuilt app</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>MULTIVARIATE DATA ANALYSIS TOOLS FOR AGRICULTURAL RESEARCH</strong>" was written by Dr. PRATHEESH P GOPINATH. It was last built on 2023-10-20.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>
</body>
</html>
